{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0db6a17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e5f691e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ö–õ–ê–°–°–ò–ß–ï–°–ö–ò–• RL –ê–ì–ï–ù–¢–û–í\n",
      "======================================================================\n",
      "\n",
      "–°—Ä–µ–¥–∞ —Å–æ–∑–¥–∞–Ω–∞:\n",
      "  Observation space: MultiDiscrete([3 3 3 3 2 3])\n",
      "  Action space: Discrete(3)\n",
      "  –î–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è: 8734 –±–∞—Ä–æ–≤\n",
      "  –î–∞–Ω–Ω—ã–µ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: 8759 –±–∞—Ä–æ–≤\n",
      "\n",
      "======================================================================\n",
      "–û–ë–£–ß–ï–ù–ò–ï –ê–ì–ï–ù–¢–û–í\n",
      "======================================================================\n",
      "\n",
      "–û–±—É—á–µ–Ω–∏–µ: SARSA(lambda)\n",
      "--------------------------------------------------\n",
      "–ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è SARSA(lambda)\n",
      "–ü–∞—Ä–∞–º–µ—Ç—Ä—ã: learning_rate=0.1, discount_factor=0.99,epsilon=1->0.01\n",
      "======================================================================\n",
      "Episode 100: Reward=-115.52, Avg100=-131.27, Epsilon=0.606, Q-table size=72, Traces size=19\n",
      "Episode 200: Reward=-208.03, Avg100=-93.73, Epsilon=0.367, Q-table size=72, Traces size=19\n",
      "======================================================================\n",
      "–û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!\n",
      "–í—Å–µ–≥–æ —ç–ø–∏–∑–æ–¥–æ–≤: 200\n",
      "–†–∞–∑–º–µ—Ä Q-—Ç–∞–±–ª–∏—Ü—ã: 72 —Å–æ—Å—Ç–æ—è–Ω–∏–π\n",
      "–°—Ä–µ–¥–Ω—è—è –Ω–∞–≥—Ä–∞–¥–∞ (–ø–æ—Å–ª–µ–¥–Ω–∏–µ 100): -93.73\n",
      "{'total_trades': 113, 'win_rate': 37.16814159292036, 'avg_pnl': -1.123264590313926, 'avg_holding_time': 3.0530973451327434, 'max_drawdown': 0.02348020190279781, 'total_pnl': -126.92889870547363, 'trades_closed_by_drawdown': 0, 'trades_closed_by_time': 0}\n"
     ]
    }
   ],
   "source": [
    "from envs.trading_env import MyTradingEnv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from agents.classical.qlearning_agent import QLearningAgent\n",
    "from agents.classical.monte_carlo_agent import MonteCarloAgent\n",
    "from agents.classical.sarsa_agent import SarsaAgent\n",
    "from agents.classical.sarsa_lambda_agent import SarsaLambdaAgent\n",
    "\n",
    "\n",
    "print(\"–¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ö–õ–ê–°–°–ò–ß–ï–°–ö–ò–• RL –ê–ì–ï–ù–¢–û–í\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "N_EPISODES = 200\n",
    "\n",
    "data_path = \"../data/data_1h_2023.csv\"\n",
    "df1 = pd.read_csv(\n",
    "    data_path,\n",
    "    index_col=0,\n",
    "    parse_dates=True,\n",
    "    date_format=\"iso8601\",\n",
    ")\n",
    "\n",
    "data_path = \"../data/data_1h_2024.csv\"\n",
    "df2 = pd.read_csv(\n",
    "    data_path,\n",
    "    index_col=0,\n",
    "    parse_dates=True,\n",
    "    date_format=\"iso8601\",\n",
    ")\n",
    "\n",
    "env = MyTradingEnv(\n",
    "    df=df1,\n",
    "    initial_balance=1000.0,\n",
    "    window_size=10,\n",
    "    max_holding_time=336,\n",
    ")\n",
    "\n",
    "print(f\"\\n–°—Ä–µ–¥–∞ —Å–æ–∑–¥–∞–Ω–∞:\")\n",
    "print(f\"  Observation space: {env.observation_space}\")\n",
    "print(f\"  Action space: {env.action_space}\")\n",
    "print(f\"  –î–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è: {len(df1)} –±–∞—Ä–æ–≤\")\n",
    "print(f\"  –î–∞–Ω–Ω—ã–µ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: {len(df2)} –±–∞—Ä–æ–≤\")\n",
    "\n",
    "agents = [\n",
    "    # QLearningAgent(),\n",
    "    # MonteCarloAgent(),\n",
    "    # SarsaAgent(),\n",
    "    SarsaLambdaAgent(lambda_param=0.8),\n",
    "]\n",
    "\n",
    "results = {}\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"–û–ë–£–ß–ï–ù–ò–ï –ê–ì–ï–ù–¢–û–í\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for agent in agents:\n",
    "    print(f\"\\n–û–±—É—á–µ–Ω–∏–µ: {agent.name}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    history = agent.train(env, n_episodes=N_EPISODES, verbose=True)\n",
    "    results[agent.name] = {\"history\": history, \"agent\": agent}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93eb4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'='*70}\")\n",
    "print(\"–¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ù–ê –ù–û–í–´–• –î–ê–ù–ù–´–•\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "test_env = MyTradingEnv(\n",
    "    df=df2,\n",
    "    initial_balance=1000.0,\n",
    "    window_size=10,\n",
    "    max_holding_time=336,\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"–°–†–ê–í–ù–ï–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–û–í –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø\")\n",
    "print(\"=\" * 70)\n",
    "print(\n",
    "    f\"{'–ê–≥–µ–Ω—Ç':<25} {'–î–æ—Ö–æ–¥–Ω–æ—Å—Ç—å':<12} {'–°–¥–µ–ª–∫–∏':<8} {'Win Rate':<10} {'–°—Ä.PnL':<10} {'Q-—Ç–∞–±–ª–∏—Ü–∞':<12}\"\n",
    ")\n",
    "print(\"-\" * 90)\n",
    "\n",
    "best_agent = None\n",
    "best_profit = -float(\"inf\")\n",
    "\n",
    "for agent_name, result in results.items():\n",
    "    agent = result[\"agent\"]\n",
    "\n",
    "    state, _ = test_env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    actions_taken = []\n",
    "\n",
    "    while not done:\n",
    "        action = agent.select_action(state, training=False)\n",
    "        actions_taken.append(action)\n",
    "        state, reward, terminated, truncated, info = test_env.step(action)\n",
    "        total_reward += reward\n",
    "        done = terminated or truncated\n",
    "\n",
    "    metrics = test_env.get_metrics()\n",
    "\n",
    "    final_portfolio = test_env._portfolio_value\n",
    "    profit_percent = ((final_portfolio / test_env.initial_balance) - 1) * 100\n",
    "\n",
    "    print(\n",
    "        f\"{agent_name:<25} {profit_percent:>7.2f}% {metrics['total_trades']:>8} {metrics['win_rate']:>9.1f}% ${metrics['avg_pnl']:>8.2f} {len(agent.q_table):>10}\"\n",
    "    )\n",
    "\n",
    "    results[agent_name][\"test_metrics\"] = {\n",
    "        \"profit_percent\": profit_percent,\n",
    "        \"final_portfolio\": final_portfolio,\n",
    "        \"total_trades\": metrics[\"total_trades\"],\n",
    "        \"win_rate\": metrics[\"win_rate\"],\n",
    "        \"avg_pnl\": metrics[\"avg_pnl\"],\n",
    "        \"total_pnl\": metrics[\"total_pnl\"],\n",
    "        \"actions_taken\": actions_taken,\n",
    "    }\n",
    "\n",
    "    if profit_percent > best_profit:\n",
    "        best_profit = profit_percent\n",
    "        best_agent = agent_name\n",
    "\n",
    "print(\"-\" * 90)\n",
    "print(f\"üèÜ –õ–£–ß–®–ò–ô –ê–ì–ï–ù–¢: {best_agent} ({best_profit:.2f}%)\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"–î–ï–¢–ê–õ–¨–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê –î–ï–ô–°–¢–í–ò–ô\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "action_names = {0: \"Hold Flat\", 1: \"Open Long\", 2: \"Close Long\"}\n",
    "\n",
    "for agent_name, result in results.items():\n",
    "    actions_taken = result[\"test_metrics\"][\"actions_taken\"]\n",
    "    action_counts = pd.Series(actions_taken).value_counts().sort_index()\n",
    "\n",
    "    print(f\"\\n{agent_name}:\")\n",
    "    total_actions = len(actions_taken)\n",
    "    for act in [0, 1, 2]:\n",
    "        count = action_counts.get(act, 0)\n",
    "        pct = count / total_actions * 100\n",
    "        print(f\"  {action_names[act]}: {count:>4} —Ä–∞–∑ ({pct:5.1f}%)\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"–°–†–ê–í–ù–ï–ù–ò–ï –ú–ï–¢–†–ò–ö –û–ë–£–ß–ï–ù–ò–Ø\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'–ê–≥–µ–Ω—Ç':<25} {'–°—Ä.–Ω–∞–≥—Ä–∞–¥–∞':<12} {'–≠–ø–∏–∑–æ–¥—ã':<10} {'–†–∞–∑–º–µ—Ä Q':<12}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for agent_name, result in results.items():\n",
    "    history = result[\"history\"]\n",
    "    episode_rewards = history[\"episode_rewards\"]\n",
    "\n",
    "    if len(episode_rewards) >= 50:\n",
    "        avg_reward = np.mean(episode_rewards[-50:])\n",
    "    else:\n",
    "        avg_reward = np.mean(episode_rewards)\n",
    "\n",
    "    agent = result[\"agent\"]\n",
    "\n",
    "    print(\n",
    "        f\"{agent_name:<25} {avg_reward:>10.2f} {agent.episode_count:>10} {len(agent.q_table):>10}\"\n",
    "    )\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"–ê–ù–ê–õ–ò–ó –¢–û–†–ì–û–í–û–ô –°–¢–†–ê–¢–ï–ì–ò–ò –õ–£–ß–®–ï–ì–û –ê–ì–ï–ù–¢–ê\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if best_agent:\n",
    "    best_result = results[best_agent]\n",
    "    metrics = best_result[\"test_metrics\"]\n",
    "\n",
    "    print(f\"–ê–≥–µ–Ω—Ç: {best_agent}\")\n",
    "    print(f\"–§–∏–Ω–∞–ª—å–Ω—ã–π –ø–æ—Ä—Ç—Ñ–µ–ª—å: ${metrics['final_portfolio']:.2f}\")\n",
    "    print(f\"–ù–∞—á–∞–ª—å–Ω—ã–π –±–∞–ª–∞–Ω—Å: ${test_env.initial_balance:.2f}\")\n",
    "    print(f\"–î–æ—Ö–æ–¥–Ω–æ—Å—Ç—å: {metrics['profit_percent']:.2f}%\")\n",
    "    print(f\"–û–±—â–∏–π PnL: ${metrics['total_pnl']:.2f}\")\n",
    "    print(f\"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–¥–µ–ª–æ–∫: {metrics['total_trades']}\")\n",
    "    print(f\"Win Rate: {metrics['win_rate']:.1f}%\")\n",
    "    print(f\"–°—Ä–µ–¥–Ω–∏–π PnL –Ω–∞ —Å–¥–µ–ª–∫—É: ${metrics['avg_pnl']:.2f}\")\n",
    "\n",
    "    actions_taken = metrics[\"actions_taken\"]\n",
    "    total_steps = len(actions_taken)\n",
    "    trading_activity = (actions_taken.count(1) / total_steps) * 100\n",
    "    print(f\"–ê–∫—Ç–∏–≤–Ω–æ—Å—Ç—å —Ç–æ—Ä–≥–æ–≤–ª–∏: {trading_activity:.1f}%\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"–†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "\n",
    "print(\"–ê–Ω–∞–ª–∏–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è:\")\n",
    "for agent_name, result in results.items():\n",
    "    train_history = result[\"history\"][\"episode_rewards\"]\n",
    "    test_profit = result[\"test_metrics\"][\"profit_percent\"]\n",
    "\n",
    "    if len(train_history) >= 50:\n",
    "        final_train_perf = np.mean(train_history[-50:])\n",
    "        if final_train_perf > 0 and test_profit < -5:\n",
    "            print(f\"  {agent_name}: –≤–æ–∑–º–æ–∂–Ω–æ–µ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ\")\n",
    "        elif test_profit > 0:\n",
    "            print(f\"  {agent_name}: —Å—Ç–∞–±–∏–ª—å–Ω–∞—è —Ä–∞–±–æ—Ç–∞\")\n",
    "        else:\n",
    "            print(f\"  {agent_name}: –Ω–∏–∑–∫–∞—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
