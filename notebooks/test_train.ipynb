{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93332704",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(\"..\") \n",
    "project_root = os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd())))\n",
    "\n",
    "import pandas as pd\n",
    "from training import TrainingManager, TrainingConfig\n",
    "\n",
    "TRAIN_VERSION = \"v2\" \n",
    "N_EPISODES_INITIAL = 3000   \n",
    "\n",
    "data_path_2021 = \"../data/data_1h_2021.csv\"\n",
    "df1 = pd.read_csv(data_path_2021, index_col=0, parse_dates=True, date_format=\"iso8601\")\n",
    "data_path_2022 = \"../data/data_1h_2022.csv\"\n",
    "df2 = pd.read_csv(data_path_2022, index_col=0, parse_dates=True, date_format=\"iso8601\")\n",
    "\n",
    "df_train = pd.concat([df1, df2.iloc[:len(df2)//2]])\n",
    "\n",
    "env_params = {\n",
    "    \"initial_balance\": 1000.0,\n",
    "    \"window_size\": 10,\n",
    "    \"commission\": 0.0001,\n",
    "    \"slippage\": 0.0001,\n",
    "    \"max_holding_time\": 72,\n",
    "    \"holding_threshold\": 24,\n",
    "    \"max_drawdown_threshold\": 0.08,\n",
    "    \"lambda_drawdown\": 0.3,\n",
    "    \"lambda_hold\": 0.05,\n",
    "    \"reward_scaling\": 1.0,\n",
    "    \"max_steps\": 1000,\n",
    "}\n",
    "\n",
    "base_train_params = {\n",
    "    \"n_episodes\": 5000,\n",
    "    \"n_episodes_start\": 0,\n",
    "    \"max_steps\": 1000,\n",
    "    \"eval_frequency\": 50,\n",
    "    \"save_frequency\": 500,\n",
    "    \"patience\": 50,\n",
    "    \"seed\": 42,\n",
    "    \"initial_balance\": 1000.0,\n",
    "    **{k: env_params[k] for k in env_params if k != \"max_steps\"}\n",
    "}\n",
    "agents_config = {\n",
    "    \"QLearning\": {\n",
    "        \"learning_rate\": 0.1,\n",
    "        \"discount_factor\": 0.95,  # Меньше для краткосрочной торговли\n",
    "        \"epsilon_start\": 1.0,\n",
    "        \"epsilon_end\": 0.05,      # Больше для exploration\n",
    "        \"epsilon_decay\": 0.9995,  # Медленнее decay\n",
    "        \"lr_decay\": 0.99995,      # Очень медленный decay LR\n",
    "        \"min_learning_rate\": 0.01,\n",
    "    },\n",
    "    \"SARSA\": {\n",
    "        \"learning_rate\": 0.1,\n",
    "        \"discount_factor\": 0.95,\n",
    "        \"epsilon_start\": 1.0,\n",
    "        \"epsilon_end\": 0.05,\n",
    "        \"epsilon_decay\": 0.9995,\n",
    "        \"lr_decay\": 0.99995,\n",
    "        \"min_learning_rate\": 0.01,\n",
    "    },\n",
    "    \"SARSA_Lambda\": {\n",
    "        \"learning_rate\": 0.05,    # Меньше для SARSA(λ)\n",
    "        \"discount_factor\": 0.95,\n",
    "        \"epsilon_start\": 1.0,\n",
    "        \"epsilon_end\": 0.05,\n",
    "        \"epsilon_decay\": 0.9995,\n",
    "        \"lr_decay\": 0.99995,\n",
    "        \"min_learning_rate\": 0.005,\n",
    "        \"lambda_param\": 0.7,      # Traces decay\n",
    "        \"replace_traces\": True,\n",
    "    },\n",
    "    \"Monte_Carlo\": {\n",
    "        \"learning_rate\": None,    # Не используется в MC\n",
    "        \"discount_factor\": 0.99,  # Выше для MC\n",
    "        \"epsilon_start\": 1.0,\n",
    "        \"epsilon_end\": 0.1,       # Больше exploration\n",
    "        \"epsilon_decay\": 0.9997,  # Еще медленнее\n",
    "        \"first_visit\": True,\n",
    "        \"use_sample_average\": False,  # Используем постоянный LR\n",
    "    }\n",
    "}\n",
    "\n",
    "def _get_agent_instance(agent_type: str, hyperparams: dict):\n",
    "    if agent_type == \"SARSA\":\n",
    "        from agents.classical.sarsa_agent import SarsaAgent\n",
    "        agent = SarsaAgent()\n",
    "    elif agent_type == \"SARSA_Lambda\":\n",
    "        from agents.classical.sarsa_lambda_agent import SarsaLambdaAgent\n",
    "        agent = SarsaLambdaAgent()\n",
    "    elif agent_type == \"QLearning\":\n",
    "        from agents.classical.qlearning_agent import QLearningAgent\n",
    "        agent = QLearningAgent()\n",
    "    elif agent_type == \"Monte_Carlo\":\n",
    "        from agents.classical.monte_carlo_agent import MonteCarloAgent\n",
    "        agent = MonteCarloAgent()\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown agent type: {agent_type}\")\n",
    "        \n",
    "    for key, value in hyperparams.items():\n",
    "        if hasattr(agent, key):\n",
    "             setattr(agent, key, value)\n",
    "    return agent\n",
    "\n",
    "\n",
    "def run_training(\n",
    "    agent_type: str,\n",
    "    df,\n",
    "    project_root: str,\n",
    "    env_params: dict,\n",
    "    train_params: dict,\n",
    "):\n",
    "    hyperparams = agents_config.get(agent_type, {})\n",
    "    agent = _get_agent_instance(agent_type, hyperparams)\n",
    "\n",
    "    config = TrainingConfig(\n",
    "        agent_name=f\"{agent_type}_{TRAIN_VERSION}\",\n",
    "        agent_type=agent_type,\n",
    "        **train_params,\n",
    "    )\n",
    "\n",
    "    manager = TrainingManager(\n",
    "        base_log_dir=os.path.join(project_root, \"training_data/logs\"),\n",
    "        base_checkpoint_dir=os.path.join(project_root, \"training_data/checkpoints\"),\n",
    "    )\n",
    "\n",
    "    experiment_name = f\"exp_{agent_type.lower()}_{TRAIN_VERSION}\"\n",
    "    print(f\" НАЧАЛО ПЕРВИЧНОГО ОБУЧЕНИЯ: {agent_type} (Ep: 0 -> {train_params['n_episodes']})\")\n",
    "    print(f\"=======================================================\")\n",
    "    return manager.train_agent(agent, df, config, experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15793a91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " НАЧАЛО ПЕРВИЧНОГО ОБУЧЕНИЯ: QLearning (Ep: 0 -> 5000)\n",
      "=======================================================\n",
      "\n",
      "================================================================================\n",
      "Начало обучения: QLearning_v2\n",
      "Эксперимент: exp_qlearning_v2\n",
      "Эпизодов: 5000\n",
      "Max steps: 1000\n",
      "Learning rate: 0.1\n",
      "Discount factor: 0.95\n",
      "Epsilon: 1.0->0.01\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Cannot index by location index with a non-integer key",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# --- ЗАПУСК QLearning ---\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mrun_training\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43magent_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mQLearning\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdf_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproject_root\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproject_root\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43menv_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43menv_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbase_train_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 131\u001b[39m, in \u001b[36mrun_training\u001b[39m\u001b[34m(agent_type, df, project_root, env_params, train_params)\u001b[39m\n\u001b[32m    129\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m НАЧАЛО ПЕРВИЧНОГО ОБУЧЕНИЯ: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00magent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (Ep: 0 -> \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_params[\u001b[33m'\u001b[39m\u001b[33mn_episodes\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    130\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m=======================================================\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmanager\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexperiment_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/d/Study/python/TradingAgentClassicRL/notebooks/../training/training_manager.py:164\u001b[39m, in \u001b[36mTrainingManager.train_agent\u001b[39m\u001b[34m(self, agent, df, config, experiment_name, verbose)\u001b[39m\n\u001b[32m    161\u001b[39m \u001b[38;5;66;03m# Основной цикл обучения\u001b[39;00m\n\u001b[32m    162\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m episode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config.n_episodes_start, config.n_episodes):\n\u001b[32m    163\u001b[39m     \u001b[38;5;66;03m# 1. ОБУЧЕНИЕ НА TRAIN ДАННЫХ\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m164\u001b[39m     state, info = \u001b[43mtrain_env\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    165\u001b[39m     done = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    166\u001b[39m     episode_reward = \u001b[32m0\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/d/Study/python/TradingAgentClassicRL/notebooks/../envs/trading_env.py:382\u001b[39m, in \u001b[36mMyTradingEnv.reset\u001b[39m\u001b[34m(self, seed, options)\u001b[39m\n\u001b[32m    379\u001b[39m \u001b[38;5;28mself\u001b[39m.portfolio_value = \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;28mself\u001b[39m.initial_balance)\n\u001b[32m    380\u001b[39m \u001b[38;5;28mself\u001b[39m.prev_portfolio_value = \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;28mself\u001b[39m.initial_balance)\n\u001b[32m--> \u001b[39m\u001b[32m382\u001b[39m obs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_observation\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    383\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m obs, {}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/d/Study/python/TradingAgentClassicRL/notebooks/../envs/trading_env.py:146\u001b[39m, in \u001b[36mMyTradingEnv._get_observation\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_observation\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> np.ndarray:\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m     row = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcurrent_step\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    147\u001b[39m     rsi_level = \u001b[38;5;28mint\u001b[39m(row[\u001b[33m\"\u001b[39m\u001b[33mrsi_discrete\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m    148\u001b[39m     macd_signal = \u001b[38;5;28mint\u001b[39m(row[\u001b[33m\"\u001b[39m\u001b[33mmacd_discrete\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rl/lib/python3.11/site-packages/pandas/core/indexing.py:1191\u001b[39m, in \u001b[36m_LocationIndexer.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   1189\u001b[39m maybe_callable = com.apply_if_callable(key, \u001b[38;5;28mself\u001b[39m.obj)\n\u001b[32m   1190\u001b[39m maybe_callable = \u001b[38;5;28mself\u001b[39m._check_deprecated_callable_usage(key, maybe_callable)\n\u001b[32m-> \u001b[39m\u001b[32m1191\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rl/lib/python3.11/site-packages/pandas/core/indexing.py:1749\u001b[39m, in \u001b[36m_iLocIndexer._getitem_axis\u001b[39m\u001b[34m(self, key, axis)\u001b[39m\n\u001b[32m   1747\u001b[39m key = item_from_zerodim(key)\n\u001b[32m   1748\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_integer(key):\n\u001b[32m-> \u001b[39m\u001b[32m1749\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCannot index by location index with a non-integer key\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1751\u001b[39m \u001b[38;5;66;03m# validate the location\u001b[39;00m\n\u001b[32m   1752\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_integer(key, axis)\n",
      "\u001b[31mTypeError\u001b[39m: Cannot index by location index with a non-integer key"
     ]
    }
   ],
   "source": [
    "# --- ЗАПУСК QLearning ---\n",
    "run_training(\n",
    "    agent_type=\"QLearning\",\n",
    "    df=df_train,\n",
    "    project_root=project_root,\n",
    "    env_params=env_params,\n",
    "    train_params=base_train_params,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f701232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- ЗАПУСК SARSA ---\n",
    "run_training(\n",
    "    agent_type=\"SARSA\",\n",
    "    df=df_train,\n",
    "    project_root=project_root,\n",
    "    env_params=env_params,\n",
    "    train_params=base_train_params,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152aad84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- ЗАПУСК SARSA_Lambda ---\n",
    "run_training(\n",
    "    agent_type=\"SARSA_Lambda\",\n",
    "    df=df_train,\n",
    "    project_root=project_root,\n",
    "    env_params=env_params,\n",
    "    train_params=base_train_params,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0613ca72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- ЗАПУСК Monte_Carlo ---\n",
    "run_training(\n",
    "    agent_type=\"Monte_Carlo\",\n",
    "    df=df_train,\n",
    "    project_root=project_root,\n",
    "    env_params=env_params,\n",
    "    train_params=base_train_params,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
