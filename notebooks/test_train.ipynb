{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93332704",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ¾ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ: 13089 ÑÑ‚Ñ€Ğ¾Ğº\n",
            "ĞŸĞµÑ€Ğ¸Ğ¾Ğ´: 28966.36 - 19390.17\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "current_dir = Path.cwd()\n",
        "if current_dir.name == \"notebooks\":\n",
        "    project_root = current_dir.parent\n",
        "    sys.path.insert(0, str(project_root))\n",
        "else:\n",
        "    project_root = Path(os.getcwd())\n",
        "    sys.path.insert(0, str(project_root))\n",
        "\n",
        "import pandas as pd\n",
        "from training import TrainingManager, TrainingConfig\n",
        "\n",
        "TRAIN_VERSION = \"v2\"\n",
        "N_EPISODES_INITIAL = 5000   \n",
        "\n",
        "data_path_2021 = project_root / \"data\" / \"data_1h_2021.csv\"\n",
        "data_path_2022 = project_root / \"data\" / \"data_1h_2022.csv\"\n",
        "\n",
        "if not data_path_2021.exists():\n",
        "    data_path_2021 = project_root.parent / \"data\" / \"data_1h_2021.csv\"\n",
        "    data_path_2022 = project_root.parent / \"data\" / \"data_1h_2022.csv\"\n",
        "\n",
        "df1 = pd.read_csv(data_path_2021, index_col=0, parse_dates=True, date_format=\"iso8601\")\n",
        "df2 = pd.read_csv(data_path_2022, index_col=0, parse_dates=True, date_format=\"iso8601\")\n",
        "\n",
        "df_train = pd.concat([df1, df2.iloc[:len(df2)//2]])\n",
        "\n",
        "print(f\"Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ¾ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ: {len(df_train)} ÑÑ‚Ñ€Ğ¾Ğº\")\n",
        "print(f\"ĞŸĞµÑ€Ğ¸Ğ¾Ğ´: {df_train.index[0]} - {df_train.index[-1]}\")\n",
        "\n",
        "env_params = {\n",
        "    \"initial_balance\": 1000.0,\n",
        "    \"window_size\": 10,\n",
        "    \"commission\": 0.0001,\n",
        "    \"slippage\": 0.0001,\n",
        "    \"max_holding_time\": 72,\n",
        "    \"max_drawdown_threshold\": 0.08,\n",
        "    \"max_steps\": 1000,\n",
        "}\n",
        "\n",
        "base_train_params = {\n",
        "    \"n_episodes\": N_EPISODES_INITIAL,\n",
        "    \"n_episodes_start\": 0,\n",
        "    \"max_steps\": 1000,\n",
        "    \"eval_frequency\": 100,\n",
        "    \"save_frequency\": 500,\n",
        "    \"patience\": 30,\n",
        "    \"seed\": 42,\n",
        "    \"initial_balance\": 1000.0,\n",
        "    **{k: env_params[k] for k in env_params if k not in [\"max_steps\"]}\n",
        "}\n",
        "\n",
        "agents_config = {\n",
        "    \"QLearning\": {\n",
        "        \"learning_rate\": 0.1,\n",
        "        \"discount_factor\": 0.95,\n",
        "        \"epsilon_start\": 1.0,\n",
        "        \"epsilon_end\": 0.01,\n",
        "        \"epsilon_decay\": 0.9998,\n",
        "    },\n",
        "    \"SARSA\": {\n",
        "        \"learning_rate\": 0.1,\n",
        "        \"discount_factor\": 0.95,\n",
        "        \"epsilon_start\": 1.0,\n",
        "        \"epsilon_end\": 0.01,\n",
        "        \"epsilon_decay\": 0.9998,\n",
        "    },\n",
        "    \"SARSA_Lambda\": {\n",
        "        \"learning_rate\": 0.1,\n",
        "        \"discount_factor\": 0.95,\n",
        "        \"epsilon_start\": 1.0,\n",
        "        \"epsilon_end\": 0.01,\n",
        "        \"epsilon_decay\": 0.9998,\n",
        "        \"lambda_param\": 0.8,\n",
        "        \"replace_traces\": True,\n",
        "    },\n",
        "    \"Monte_Carlo\": {\n",
        "        \"learning_rate\": 0.1,\n",
        "        \"discount_factor\": 0.95,\n",
        "        \"epsilon_start\": 1.0,\n",
        "        \"epsilon_end\": 0.01,\n",
        "        \"epsilon_decay\": 0.9998,\n",
        "        \"first_visit\": True,\n",
        "        \"use_sample_average\": False,\n",
        "    }\n",
        "}\n",
        "\n",
        "def _get_agent_instance(agent_type: str, hyperparams: dict):\n",
        "    if agent_type == \"SARSA\":\n",
        "        from agents.classical.sarsa_agent import SarsaAgent\n",
        "        agent = SarsaAgent()\n",
        "    elif agent_type == \"SARSA_Lambda\":\n",
        "        from agents.classical.sarsa_lambda_agent import SarsaLambdaAgent\n",
        "        agent = SarsaLambdaAgent()\n",
        "    elif agent_type == \"QLearning\":\n",
        "        from agents.classical.qlearning_agent import QLearningAgent\n",
        "        agent = QLearningAgent()\n",
        "    elif agent_type == \"Monte_Carlo\":\n",
        "        from agents.classical.monte_carlo_agent import MonteCarloAgent\n",
        "        agent = MonteCarloAgent()\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown agent type: {agent_type}\")\n",
        "        \n",
        "    for key, value in hyperparams.items():\n",
        "        if hasattr(agent, key):\n",
        "            setattr(agent, key, value)\n",
        "        else:\n",
        "            print(f\"ĞŸÑ€ĞµĞ´ÑƒĞ¿Ñ€ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ğµ: Ğ°Ğ³ĞµĞ½Ñ‚ {agent_type} Ğ½Ğµ Ğ¸Ğ¼ĞµĞµÑ‚ Ğ°Ñ‚Ñ€Ğ¸Ğ±ÑƒÑ‚Ğ° {key}\")\n",
        "    \n",
        "    return agent\n",
        "\n",
        "\n",
        "def run_training(\n",
        "    agent_type: str,\n",
        "    df: pd.DataFrame,\n",
        "    project_root: Path,\n",
        "    env_params: dict,\n",
        "    train_params: dict,\n",
        "    verbose: bool = True,\n",
        "):\n",
        "    hyperparams = agents_config.get(agent_type, {})\n",
        "    agent = _get_agent_instance(agent_type, hyperparams)\n",
        "\n",
        "    config = TrainingConfig(\n",
        "        agent_name=f\"{agent_type}_{TRAIN_VERSION}\",\n",
        "        agent_type=agent_type,\n",
        "        **train_params,\n",
        "    )\n",
        "\n",
        "    manager = TrainingManager(\n",
        "        base_log_dir=str(project_root / \"training_data\" / \"logs\"),\n",
        "        base_checkpoint_dir=str(project_root / \"training_data\" / \"checkpoints\"),\n",
        "        seed=train_params.get(\"seed\", 42),\n",
        "    )\n",
        "\n",
        "    experiment_name = f\"exp_{agent_type.lower()}_{TRAIN_VERSION}\"\n",
        "    \n",
        "    if verbose:\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"ĞĞĞ§ĞĞ›Ğ ĞŸĞ•Ğ Ğ’Ğ˜Ğ§ĞĞĞ“Ğ ĞĞ‘Ğ£Ğ§Ğ•ĞĞ˜Ğ¯: {agent_type}\")\n",
        "        print(f\"{'='*80}\")\n",
        "        print(f\"Ğ­Ğ¿Ğ¸Ğ·Ğ¾Ğ´Ğ¾Ğ²: {train_params['n_episodes']}\")\n",
        "        print(f\"Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚: {experiment_name}\")\n",
        "        print(f\"{'='*80}\\n\")\n",
        "    \n",
        "    return manager.train_agent(agent, df, config, experiment_name, verbose=verbose)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "15793a91",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "ĞĞĞ§ĞĞ›Ğ ĞŸĞ•Ğ Ğ’Ğ˜Ğ§ĞĞĞ“Ğ ĞĞ‘Ğ£Ğ§Ğ•ĞĞ˜Ğ¯: QLearning\n",
            "================================================================================\n",
            "Ğ­Ğ¿Ğ¸Ğ·Ğ¾Ğ´Ğ¾Ğ²: 5000\n",
            "Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚: exp_qlearning_v1\n",
            "================================================================================\n",
            "\n",
            "\n",
            "====================================================================================================\n",
            "ğŸš€ ĞĞĞ§ĞĞ›Ğ ĞĞ‘Ğ£Ğ§Ğ•ĞĞ˜Ğ¯\n",
            "====================================================================================================\n",
            "ĞĞ³ĞµĞ½Ñ‚:          QLearning_v1\n",
            "Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚:    exp_qlearning_v1\n",
            "Ğ­Ğ¿Ğ¸Ğ·Ğ¾Ğ´Ğ¾Ğ²:       5000\n",
            "Max steps:       1000\n",
            "Learning rate:  0.1\n",
            "Discount:       0.95\n",
            "Epsilon:        1.0 â†’ 0.01\n",
            "Eval frequency: 100\n",
            "Patience:       30\n",
            "Initial balance: $1,000.00\n",
            "====================================================================================================\n",
            "\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrun_training\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43magent_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mQLearning\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdf_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproject_root\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproject_root\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43menv_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_train_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[2], line 149\u001b[0m, in \u001b[0;36mrun_training\u001b[0;34m(agent_type, df, project_root, env_params, train_params, verbose)\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mĞ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexperiment_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m80\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 149\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmanager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexperiment_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/mnt/d/Study/Code/ml/TradingAgentClassicRL/training/training_manager.py:191\u001b[0m, in \u001b[0;36mTrainingManager.train_agent\u001b[0;34m(self, agent, df, config, experiment_name, verbose)\u001b[0m\n\u001b[1;32m    188\u001b[0m done \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[0;32m--> 191\u001b[0m     next_action \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    193\u001b[0m     next_action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m/mnt/d/Study/Code/ml/TradingAgentClassicRL/agents/classical/base_classical_agent.py:60\u001b[0m, in \u001b[0;36mBaseClassicalAgent.select_action\u001b[0;34m(self, state, training)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m training:\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_best_action(state)\n\u001b[0;32m---> 60\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrand\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepsilon:\n\u001b[1;32m     61\u001b[0m     action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_actions)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "run_training(\n",
        "    agent_type=\"QLearning\",\n",
        "    df=df_train,\n",
        "    project_root=project_root,\n",
        "    env_params=env_params,\n",
        "    train_params=base_train_params,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f701232",
      "metadata": {},
      "outputs": [],
      "source": [
        "run_training(\n",
        "    agent_type=\"SARSA\",\n",
        "    df=df_train,\n",
        "    project_root=project_root,\n",
        "    env_params=env_params,\n",
        "    train_params=base_train_params,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "152aad84",
      "metadata": {},
      "outputs": [],
      "source": [
        "run_training(\n",
        "    agent_type=\"SARSA_Lambda\",\n",
        "    df=df_train,\n",
        "    project_root=project_root,\n",
        "    env_params=env_params,\n",
        "    train_params=base_train_params,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0613ca72",
      "metadata": {},
      "outputs": [],
      "source": [
        "run_training(\n",
        "    agent_type=\"Monte_Carlo\",\n",
        "    df=df_train,\n",
        "    project_root=project_root,\n",
        "    env_params=env_params,\n",
        "    train_params=base_train_params,\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "rl",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
