{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db6a17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import os\n",
    "project_root = os.path.dirname(os.path.dirname(os.path.abspath(\"__file__\")))\n",
    "\n",
    "import pandas as pd\n",
    "from training import TrainingManager, TrainingConfig\n",
    "\n",
    "\n",
    "TRAIN_VERSION=\"v2\"\n",
    "\n",
    "data_path = \"../data/data_1h_2021.csv\"\n",
    "df1 = pd.read_csv(data_path, index_col=0, parse_dates=True, date_format=\"iso8601\")\n",
    "data_path = \"../data/data_1h_2022.csv\"\n",
    "df2 = pd.read_csv(data_path, index_col=0, parse_dates=True, date_format=\"iso8601\")\n",
    "data_path = \"../data/data_1h_2023.csv\"\n",
    "df3 = pd.read_csv(data_path, index_col=0, parse_dates=True, date_format=\"iso8601\")\n",
    "\n",
    "env_params = {\n",
    "    \"initial_balance\": 1000.0,\n",
    "    \"window_size\": 10,\n",
    "    \"commission\": 0.0001,\n",
    "    \"slippage\": 0.0001,\n",
    "    \"max_holding_time\": 48,\n",
    "    \"holding_threshold\": 12, \n",
    "    \"max_drawdown_threshold\": 0.05,\n",
    "    \"lambda_drawdown\": 0.5,\n",
    "    \"lambda_hold\": 0.1,\n",
    "    \"reward_scaling\": 1.0, \n",
    "    \"max_steps\": 1000\n",
    "}\n",
    "\n",
    "df_train = pd.concat([df1, df2.iloc[:len(df2)//2]])\n",
    "\n",
    "\n",
    "base_train_params = {\n",
    "    \"n_episodes\": 3000,\n",
    "    \"n_episodes_start\": 0,\n",
    "    \"max_steps\": 1000,\n",
    "    \"eval_frequency\": 100,\n",
    "    \"save_frequency\": 500,\n",
    "    \"patience\": 200,\n",
    "    \"initial_balance\": 1000.0,\n",
    "\n",
    "    \"window_size\": 10,\n",
    "    \"commission\": 0.0001,\n",
    "    \"slippage\": 0.0001,\n",
    "    \"max_holding_time\": 48,\n",
    "    \"holding_threshold\": 12,\n",
    "    \"max_drawdown_threshold\": 0.05,\n",
    "    \"lambda_drawdown\": 0.5,\n",
    "    \"lambda_hold\": 0.1,\n",
    "    \"reward_scaling\": 1.0,\n",
    "}\n",
    "\n",
    "\n",
    "agents_config = {\n",
    "    \"QLearning\": {\n",
    "        \"learning_rate\": 0.05,\n",
    "        \"discount_factor\": 0.99,\n",
    "        \"epsilon_start\": 1.0,\n",
    "        \"epsilon_end\": 0.01,\n",
    "        \"epsilon_decay\": 0.998,\n",
    "    },\n",
    "    \"SARSA\": {\n",
    "        \"learning_rate\": 0.05,\n",
    "        \"discount_factor\": 0.99,\n",
    "        \"epsilon_start\": 1.0,\n",
    "        \"epsilon_end\": 0.01,\n",
    "        \"epsilon_decay\": 0.998,\n",
    "    },\n",
    "    \"SARSA_Lambda\": {\n",
    "        \"learning_rate\": 0.02, \n",
    "        \"discount_factor\": 0.99,\n",
    "        \"epsilon_start\": 1.0,\n",
    "        \"epsilon_end\": 0.01,\n",
    "        \"epsilon_decay\": 0.998,\n",
    "        \"trace_decay\": 0.9,\n",
    "    },\n",
    "    \"Monte_Carlo\": {\n",
    "        \"learning_rate\": 0.02,\n",
    "        \"discount_factor\": 0.99,\n",
    "        \"epsilon_start\": 1.0,\n",
    "        \"epsilon_end\": 0.05,  \n",
    "        \"epsilon_decay\": 0.999, \n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f5e3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(\n",
    "    agent_type: str,\n",
    "    df,\n",
    "    project_root: str,\n",
    "    env_params: dict,\n",
    "    train_params: dict,\n",
    "):\n",
    "\n",
    "    if agent_type == \"SARSA\":\n",
    "        from agents.classical.sarsa_agent import SarsaAgent\n",
    "\n",
    "        agent = SarsaAgent()\n",
    "\n",
    "    elif agent_type == \"SARSA_Lambda\":\n",
    "        from agents.classical.sarsa_lambda_agent import SarsaLambdaAgent\n",
    "\n",
    "        agent = SarsaLambdaAgent()\n",
    "\n",
    "    elif agent_type == \"QLearning\":\n",
    "        from agents.classical.qlearning_agent import QLearningAgent\n",
    "\n",
    "        agent = QLearningAgent()\n",
    "\n",
    "    elif agent_type == \"Monte_Carlo\":\n",
    "        from agents.classical.monte_carlo_agent import MonteCarloAgent\n",
    "\n",
    "        agent = MonteCarloAgent()\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown agent type: {agent_type}\")\n",
    "\n",
    "    config = TrainingConfig(\n",
    "        agent_name=f\"{agent_type}_{TRAIN_VERSION}\",\n",
    "        agent_type=agent_type,\n",
    "        **train_params,\n",
    "    )\n",
    "\n",
    "    manager = TrainingManager(\n",
    "        base_log_dir=os.path.join(project_root, \"training_data/logs\"),\n",
    "        base_checkpoint_dir=os.path.join(project_root, \"training_data/checkpoints\"),\n",
    "    )\n",
    "\n",
    "    experiment_name = f\"exp_{agent_type.lower()}_{TRAIN_VERSION}\"\n",
    "    return manager.train_agent(agent, df, config, experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe78877",
   "metadata": {},
   "source": [
    "### Обучение QLearningAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5f691e",
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECTED_AGENT = \"QLearning\"\n",
    "\n",
    "final_train_params = {**base_train_params, **agents_config[SELECTED_AGENT]}\n",
    "\n",
    "result = run_training(\n",
    "    agent_type=SELECTED_AGENT,\n",
    "    df=df_train,\n",
    "    project_root=project_root,\n",
    "    env_params=env_params,\n",
    "    train_params=final_train_params\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe6f361",
   "metadata": {},
   "source": [
    "### Обучение MonteCarloAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e346551c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECTED_AGENT = \"Monte_Carlo\"\n",
    "\n",
    "final_train_params = {**base_train_params, **agents_config[SELECTED_AGENT]}\n",
    "\n",
    "result = run_training(\n",
    "    agent_type=SELECTED_AGENT,\n",
    "    df=df_train,\n",
    "    project_root=project_root,\n",
    "    env_params=env_params,\n",
    "    train_params=final_train_params\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20efa51f",
   "metadata": {},
   "source": [
    "### Обучение SarsaAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3131b50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECTED_AGENT = \"SARSA\"\n",
    "\n",
    "final_train_params = {**base_train_params, **agents_config[SELECTED_AGENT]}\n",
    "\n",
    "result = run_training(\n",
    "    agent_type=SELECTED_AGENT,\n",
    "    df=df_train,\n",
    "    project_root=project_root,\n",
    "    env_params=env_params,\n",
    "    train_params=final_train_params\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a891c4d7",
   "metadata": {},
   "source": [
    "### Обучение SarsaLambdaAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd85913",
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECTED_AGENT = \"SARSA_Lambda\"\n",
    "\n",
    "final_train_params = {**base_train_params, **agents_config[SELECTED_AGENT]}\n",
    "\n",
    "result = run_training(\n",
    "    agent_type=SELECTED_AGENT,\n",
    "    df=df_train,\n",
    "    project_root=project_root,\n",
    "    env_params=env_params,\n",
    "    train_params=final_train_params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93eb4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "WINDOW_SIZE = 100 \n",
    "\n",
    "experiment_dirs = [\n",
    "    f\"../training_data/logs/exp_qlearning_{TRAIN_VERSION}\",\n",
    "    f\"../training_data/logs/exp_monte_carlo_{TRAIN_VERSION}\",\n",
    "    f\"../training_data/logs/exp_sarsa_{TRAIN_VERSION}\",\n",
    "    f\"../training_data/logs/exp_sarsa_lambda_{TRAIN_VERSION}\",\n",
    "]\n",
    "\n",
    "all_data = []\n",
    "for exp_dir_str in experiment_dirs:\n",
    "    exp_dir = Path(exp_dir_str)\n",
    "\n",
    "    episodes_df = pd.read_csv(exp_dir / \"episodes.csv\")\n",
    "    \n",
    "    with open(exp_dir / \"training_summary.json\") as f:\n",
    "        summary = json.load(f)\n",
    "    agent_raw_name = summary[\"config\"][\"agent_name\"]\n",
    "    agent_name = agent_raw_name.split('_')[0] \n",
    "    \n",
    "    episodes_df[\"agent\"] = agent_name\n",
    "    all_data.append(episodes_df)\n",
    "\n",
    "df = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "agents = df[\"agent\"].unique()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "ax1 = axes[0]\n",
    "for agent in agents:\n",
    "    agent_data = df[df[\"agent\"] == agent]\n",
    "    smoothed = agent_data[\"reward\"].rolling(window=WINDOW_SIZE, min_periods=1).mean()\n",
    "    ax1.plot(agent_data[\"episode\"], smoothed, label=agent, linewidth=2)\n",
    "\n",
    "ax1.set_title(f\"Сравнение средней награды (сглаживание {WINDOW_SIZE})\", fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel(\"Эпизод\")\n",
    "ax1.set_ylabel(\"Reward (сглаженная)\")\n",
    "ax1.legend(title=\"Агент\")\n",
    "ax1.grid(True, alpha=0.5)\n",
    "\n",
    "ax2 = axes[1]\n",
    "for agent in agents:\n",
    "    agent_data = df[df[\"agent\"] == agent]\n",
    "    smoothed_balance = agent_data[\"portfolio_value\"].rolling(window=WINDOW_SIZE, min_periods=1).mean()\n",
    "    ax2.plot(agent_data[\"episode\"], smoothed_balance, label=agent, linewidth=2)\n",
    "\n",
    "INITIAL_BALANCE = 1000.0\n",
    "ax2.axhline(y=INITIAL_BALANCE, color='black', linestyle='--', alpha=0.7, label='Начальный баланс')\n",
    "\n",
    "ax2.set_title(f\"Динамика стоимости портфеля (сглаживание {WINDOW_SIZE})\", fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel(\"Эпизод\")\n",
    "ax2.set_ylabel(\"Portfolio Value ($)\")\n",
    "ax2.legend(title=\"Агент\")\n",
    "ax2.grid(True, alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "final_metrics = (\n",
    "    df.groupby(\"agent\")\n",
    "    .tail(WINDOW_SIZE)\n",
    "    .groupby(\"agent\")\n",
    "    [[\"reward\", \"portfolio_value\", \"win_rate\", \"n_trades\", \"max_drawdown\"]]\n",
    "    .mean()\n",
    ")\n",
    "\n",
    "final_metrics.columns = [\n",
    "    \"Reward (Avg)\", \n",
    "    \"Balance $ (Avg)\", \n",
    "    \"Win Rate % (Avg)\", \n",
    "    \"Trades (Avg)\", \n",
    "    \"Max DD (Avg)\"\n",
    "]\n",
    "\n",
    "final_metrics = final_metrics.sort_values(\"Reward (Avg)\", ascending=False)\n",
    "print(f\" Финальные результаты (среднее за последние {WINDOW_SIZE} эпизодов)\")\n",
    "\n",
    "display(final_metrics.round(2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
