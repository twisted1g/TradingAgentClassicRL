{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db6a17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import os\n",
    "\n",
    "project_root = os.path.dirname(os.path.dirname(os.path.abspath(\"__file__\")))\n",
    "\n",
    "import pandas as pd\n",
    "from training import TrainingManager, TrainingConfig\n",
    "from envs.trading_env import MyTradingEnv\n",
    "\n",
    "N_EPISODES=20_000\n",
    "\n",
    "MAX_STEPS=1000\n",
    "LEARNING_RATE=0.1\n",
    "DISCOUNT_FACTOR=0.99\n",
    "EPSILON_START=1.0\n",
    "EPSILON_END=0.01\n",
    "EPSILON_DECAY=0.995\n",
    "EVAL_FREQUANCY=500\n",
    "SAVE_FREQUANCY=1_000\n",
    "\n",
    "TRAIN_VERSION=\"v3\"\n",
    "\n",
    "data_path = \"../data/data_1h_2023.csv\"\n",
    "df1 = pd.read_csv(data_path, index_col=0, parse_dates=True, date_format=\"iso8601\")\n",
    "\n",
    "data_path = \"../data/data_1h_2024.csv\"\n",
    "df2 = pd.read_csv(data_path, index_col=0, parse_dates=True, date_format=\"iso8601\")\n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "\n",
    "INITIAL_BALANCE = 1000.0\n",
    "WINDOW_SIZE = 10\n",
    "COMMISSION = 0.0001\n",
    "SLIPPAGE = 0.0005\n",
    "MAX_HOLDING_TIME = 60 * 24\n",
    "HOLDING_THRESHOLD = 24\n",
    "MAX_DRAWDOWN_THRESHOLD = 0.05\n",
    "LAMBDA_DRAWDOWN = 0.1 # напрада в процентах\n",
    "LAMBDA_HOLD = 0.025 # напрада в процентах\n",
    "REWARD_SCALING=100.0 # напрада в процентах"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe78877",
   "metadata": {},
   "source": [
    "### Обучение QLearningAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5f691e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from agents.classical.qlearning_agent import QLearningAgent\n",
    "\n",
    "agent = QLearningAgent()\n",
    "env = MyTradingEnv(\n",
    "    df=df,\n",
    "    initial_balance=INITIAL_BALANCE,\n",
    "    window_size=WINDOW_SIZE,\n",
    "    commission=COMMISSION,\n",
    "    slippage=SLIPPAGE,\n",
    "    max_holding_time=MAX_HOLDING_TIME,\n",
    "    lambda_drawdown=LAMBDA_DRAWDOWN,\n",
    "    lambda_hold=LAMBDA_HOLD,\n",
    "    reward_scaling=REWARD_SCALING,\n",
    ")\n",
    "\n",
    "config = TrainingConfig(\n",
    "    agent_name=f\"QLearning_{TRAIN_VERSION}\",\n",
    "        agent_type=\"QLearning\",\n",
    "        n_episodes=N_EPISODES,\n",
    "        max_steps=MAX_STEPS,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        discount_factor=DISCOUNT_FACTOR,\n",
    "        epsilon_start=EPSILON_START,\n",
    "        epsilon_end=EPSILON_END,\n",
    "        epsilon_decay=EPSILON_DECAY,\n",
    "        eval_frequency=EVAL_FREQUANCY,\n",
    "        save_frequency=SAVE_FREQUANCY\n",
    ")\n",
    "\n",
    "manager = TrainingManager(\n",
    "    base_log_dir=os.path.join(project_root, \"training_data/logs\"),\n",
    "    base_checkpoint_dir=os.path.join(project_root, \"training_data/checkpoints\")\n",
    ")\n",
    "results = manager.train_agent(agent, env, config, experiment_name=f\"exp_qlearn_{TRAIN_VERSION}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe6f361",
   "metadata": {},
   "source": [
    "### Обучение MonteCarloAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e346551c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents.classical.monte_carlo_agent import MonteCarloAgent\n",
    "\n",
    "agent = MonteCarloAgent()\n",
    "env = MyTradingEnv(\n",
    "    df=df,\n",
    "    initial_balance=INITIAL_BALANCE,\n",
    "    window_size=WINDOW_SIZE,\n",
    "    commission=COMMISSION,\n",
    "    slippage=SLIPPAGE,\n",
    "    max_holding_time=MAX_HOLDING_TIME,\n",
    "    lambda_drawdown=LAMBDA_DRAWDOWN,\n",
    "    lambda_hold=LAMBDA_HOLD,\n",
    "    reward_scaling=REWARD_SCALING,\n",
    ")\n",
    "\n",
    "config = TrainingConfig(\n",
    "    agent_name=f\"MonteCarlo_{TRAIN_VERSION}\",\n",
    "        agent_type=\"MonteCarlo\",\n",
    "        n_episodes=N_EPISODES,\n",
    "        max_steps=100,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        discount_factor=DISCOUNT_FACTOR,\n",
    "        epsilon_start=EPSILON_START,\n",
    "        epsilon_end=EPSILON_END,\n",
    "        epsilon_decay=EPSILON_DECAY,\n",
    "        eval_frequency=EVAL_FREQUANCY,\n",
    "        save_frequency=SAVE_FREQUANCY\n",
    ")\n",
    "\n",
    "manager = TrainingManager(\n",
    "    base_log_dir=os.path.join(project_root, \"training_data/logs\"),\n",
    "    base_checkpoint_dir=os.path.join(project_root, \"training_data/checkpoints\")\n",
    ")\n",
    "results = manager.train_agent(agent, env, config, experiment_name=f\"exp_monte_carlo_{TRAIN_VERSION}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20efa51f",
   "metadata": {},
   "source": [
    "### Обучение SarsaAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3131b50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents.classical.sarsa_agent import SarsaAgent\n",
    "\n",
    "agent = SarsaAgent()\n",
    "env = MyTradingEnv(\n",
    "    df=df,\n",
    "    initial_balance=INITIAL_BALANCE,\n",
    "    window_size=WINDOW_SIZE,\n",
    "    commission=COMMISSION,\n",
    "    slippage=SLIPPAGE,\n",
    "    max_holding_time=MAX_HOLDING_TIME,\n",
    "    lambda_drawdown=LAMBDA_DRAWDOWN,\n",
    "    lambda_hold=LAMBDA_HOLD,\n",
    "    reward_scaling=REWARD_SCALING,\n",
    ")\n",
    "\n",
    "config = TrainingConfig(\n",
    "    agent_name=f\"SARSA_{TRAIN_VERSION}\",\n",
    "        agent_type=\"SARSA\",\n",
    "        n_episodes=N_EPISODES,\n",
    "        max_steps=MAX_STEPS,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        discount_factor=DISCOUNT_FACTOR,\n",
    "        epsilon_start=EPSILON_START,\n",
    "        epsilon_end=EPSILON_END,\n",
    "        epsilon_decay=EPSILON_DECAY,\n",
    "        eval_frequency=EVAL_FREQUANCY,\n",
    "        save_frequency=SAVE_FREQUANCY\n",
    ")\n",
    "\n",
    "manager = TrainingManager(\n",
    "    base_log_dir=os.path.join(project_root, \"training_data/logs\"),\n",
    "    base_checkpoint_dir=os.path.join(project_root, \"training_data/checkpoints\")\n",
    ")\n",
    "results = manager.train_agent(agent, env, config, experiment_name=f\"exp_sarsa_{TRAIN_VERSION}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a891c4d7",
   "metadata": {},
   "source": [
    "### Обучение SarsaLambdaAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd85913",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents.classical.sarsa_lambda_agent import SarsaLambdaAgent\n",
    "\n",
    "agent = SarsaLambdaAgent()\n",
    "env = MyTradingEnv(\n",
    "    df=df,\n",
    "    initial_balance=INITIAL_BALANCE,\n",
    "    window_size=WINDOW_SIZE,\n",
    "    commission=COMMISSION,\n",
    "    slippage=SLIPPAGE,\n",
    "    max_holding_time=MAX_HOLDING_TIME,\n",
    "    lambda_drawdown=LAMBDA_DRAWDOWN,\n",
    "    lambda_hold=LAMBDA_HOLD,\n",
    "    reward_scaling=REWARD_SCALING,\n",
    ")\n",
    "\n",
    "config = TrainingConfig(\n",
    "    agent_name=f\"SARSALambda_{TRAIN_VERSION}\",\n",
    "        agent_type=\"SARSALambda\",\n",
    "        n_episodes=N_EPISODES,\n",
    "        max_steps=MAX_STEPS,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        discount_factor=DISCOUNT_FACTOR,\n",
    "        epsilon_start=EPSILON_START,\n",
    "        epsilon_end=EPSILON_END,\n",
    "        epsilon_decay=EPSILON_DECAY,\n",
    "        eval_frequency=EVAL_FREQUANCY,\n",
    "        save_frequency=SAVE_FREQUANCY,\n",
    "        lambda_param=0.5,\n",
    ")\n",
    "\n",
    "manager = TrainingManager(\n",
    "    base_log_dir=os.path.join(project_root, \"training_data/logs\"),\n",
    "    base_checkpoint_dir=os.path.join(project_root, \"training_data/checkpoints\")\n",
    ")\n",
    "results = manager.train_agent(agent, env, config, experiment_name=f\"exp_sarsa_lambda_{TRAIN_VERSION}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93eb4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "experiment_dirs = [\n",
    "    f\"../training_data/logs/exp_qlearn_{TRAIN_VERSION}\",\n",
    "    f\"../training_data/logs/exp_monte_carlo_{TRAIN_VERSION}\",\n",
    "    f\"../training_data/logs/exp_sarsa_{TRAIN_VERSION}\",\n",
    "    f\"../training_data/logs/exp_sarsa_lambda_{TRAIN_VERSION}\",\n",
    "]\n",
    "\n",
    "all_data = []\n",
    "for exp_dir in experiment_dirs:\n",
    "    exp_dir = Path(exp_dir)\n",
    "    episodes_df = pd.read_csv(exp_dir / \"episodes.csv\")\n",
    "    with open(exp_dir / \"training_summary.json\") as f:\n",
    "        summary = json.load(f)\n",
    "    agent_name = summary[\"config\"][\"agent_name\"]\n",
    "    episodes_df[\"agent\"] = agent_name\n",
    "    all_data.append(episodes_df)\n",
    "\n",
    "\n",
    "df = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "for agent in df[\"agent\"].unique():\n",
    "    agent_data = df[df[\"agent\"] == agent]\n",
    "    smoothed = agent_data[\"reward\"].rolling(window=50, min_periods=1).mean()\n",
    "    plt.plot(agent_data[\"episode\"], smoothed, label=agent)\n",
    "\n",
    "plt.title(\"Сравнение средней награды (скользящее окно = 50)\")\n",
    "plt.xlabel(\"Эпизод\")\n",
    "plt.ylabel(\"Награда (сглаженная)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "for agent in df[\"agent\"].unique():\n",
    "    agent_data = df[df[\"agent\"] == agent]\n",
    "    plt.plot(agent_data[\"episode\"], agent_data[\"portfolio_value\"], label=agent)\n",
    "\n",
    "plt.title(\"Динамика стоимости портфеля\")\n",
    "plt.xlabel(\"Эпизод\")\n",
    "plt.ylabel(\"Portfolio Value ($)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "final_metrics = df.groupby(\"agent\").tail(1)[\n",
    "    [\"agent\", \"reward\", \"portfolio_value\", \"win_rate\", \"n_trades\", \"max_drawdown\"]\n",
    "]\n",
    "display(final_metrics.round(2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
