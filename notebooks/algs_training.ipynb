{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db6a17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import os\n",
    "project_root = os.path.dirname(os.path.dirname(os.path.abspath(\"__file__\")))\n",
    "\n",
    "import pandas as pd\n",
    "from training import TrainingManager, TrainingConfig\n",
    "from envs.trading_env import MyTradingEnv\n",
    "\n",
    "N_EPISODES=4_000\n",
    "\n",
    "MAX_STEPS=2_000\n",
    "LEARNING_RATE=0.05\n",
    "DISCOUNT_FACTOR=0.9\n",
    "EPSILON_START=1.0\n",
    "EPSILON_END=0.01\n",
    "EPSILON_DECAY=0.9998\n",
    "EVAL_FREQUANCY=200\n",
    "SAVE_FREQUANCY=1_000\n",
    "\n",
    "TRAIN_VERSION=\"v1\"\n",
    "\n",
    "data_path = \"../data/data_1h_2021.csv\"\n",
    "df1 = pd.read_csv(data_path, index_col=0, parse_dates=True, date_format=\"iso8601\")\n",
    "data_path = \"../data/data_1h_2022.csv\"\n",
    "df2 = pd.read_csv(data_path, index_col=0, parse_dates=True, date_format=\"iso8601\")\n",
    "data_path = \"../data/data_1h_2023.csv\"\n",
    "df3 = pd.read_csv(data_path, index_col=0, parse_dates=True, date_format=\"iso8601\")\n",
    "\n",
    "df = pd.concat([df1])\n",
    "\n",
    "INITIAL_BALANCE = 1000.0\n",
    "WINDOW_SIZE = 10\n",
    "COMMISSION = 0.0001\n",
    "SLIPPAGE = 0.0005\n",
    "MAX_HOLDING_TIME = 60 * 24\n",
    "HOLDING_THRESHOLD = 24\n",
    "MAX_DRAWDOWN_THRESHOLD = 0.05\n",
    "LAMBDA_DRAWDOWN = 0.05\n",
    "LAMBDA_HOLD = 0.005\n",
    "REWARD_SCALING=1000.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f5e3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(\n",
    "    agent_type: str,\n",
    "    df,\n",
    "    project_root: str,\n",
    "    env_params: dict,\n",
    "    train_params: dict,\n",
    "):\n",
    "\n",
    "    if agent_type == \"SARSA\":\n",
    "        from agents.classical.sarsa_agent import SarsaAgent\n",
    "\n",
    "        agent = SarsaAgent()\n",
    "\n",
    "    elif agent_type == \"SARSA_Lambda\":\n",
    "        from agents.classical.sarsa_lambda_agent import SarsaLambdaAgent\n",
    "\n",
    "        agent = SarsaLambdaAgent()\n",
    "\n",
    "    elif agent_type == \"QLearning\":\n",
    "        from agents.classical.qlearning_agent import QLearningAgent\n",
    "\n",
    "        agent = QLearningAgent()\n",
    "\n",
    "    elif agent_type == \"Monte_Carlo\":\n",
    "        from agents.classical.monte_carlo_agent import MonteCarloAgent\n",
    "\n",
    "        agent = MonteCarloAgent()\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown agent type: {agent_type}\")\n",
    "\n",
    "    env = MyTradingEnv(df=df, **env_params)\n",
    "\n",
    "    config = TrainingConfig(\n",
    "        agent_name=f\"{agent_type}_{TRAIN_VERSION}\",\n",
    "        agent_type=agent_type,\n",
    "        **train_params,\n",
    "    )\n",
    "\n",
    "    manager = TrainingManager(\n",
    "        base_log_dir=os.path.join(project_root, \"training_data/logs\"),\n",
    "        base_checkpoint_dir=os.path.join(project_root, \"training_data/checkpoints\"),\n",
    "    )\n",
    "\n",
    "    experiment_name = f\"exp_{agent_type.lower()}_{TRAIN_VERSION}\"\n",
    "    return manager.train_agent(agent, env, config, experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcccccb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_params = {\n",
    "    \"initial_balance\": INITIAL_BALANCE,\n",
    "    \"window_size\": WINDOW_SIZE,\n",
    "    \"commission\": COMMISSION,\n",
    "    \"slippage\": SLIPPAGE,\n",
    "    \"max_holding_time\": MAX_HOLDING_TIME,\n",
    "    \"lambda_drawdown\": LAMBDA_DRAWDOWN,\n",
    "    \"lambda_hold\": LAMBDA_HOLD,\n",
    "    \"reward_scaling\": REWARD_SCALING,\n",
    "    \"max_steps\": MAX_STEPS,\n",
    "}\n",
    "\n",
    "train_params = {\n",
    "    \"n_episodes\": N_EPISODES,\n",
    "    \"max_steps\": MAX_STEPS,\n",
    "    \"learning_rate\": LEARNING_RATE,\n",
    "    \"discount_factor\": DISCOUNT_FACTOR,\n",
    "    \"epsilon_start\": EPSILON_START,\n",
    "    \"epsilon_end\": EPSILON_END,\n",
    "    \"epsilon_decay\": EPSILON_DECAY,\n",
    "    \"eval_frequency\": EVAL_FREQUANCY,\n",
    "    \"save_frequency\": SAVE_FREQUANCY,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe78877",
   "metadata": {},
   "source": [
    "### Обучение QLearningAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e5f691e",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[32m/tmp/ipykernel_20807/1570441133.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m results = run_training(\n\u001b[32m      2\u001b[39m     agent_type=\u001b[33m\"QLearning\"\u001b[39m,\n\u001b[32m      3\u001b[39m     df=df,\n\u001b[32m      4\u001b[39m     project_root=project_root,\n",
      "\u001b[32m/tmp/ipykernel_20807/218076327.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(agent_type, df, project_root, env_params, train_params)\u001b[39m\n\u001b[32m     42\u001b[39m         base_checkpoint_dir=os.path.join(project_root, \u001b[33m\"training_data/checkpoints\"\u001b[39m),\n\u001b[32m     43\u001b[39m     )\n\u001b[32m     44\u001b[39m \n\u001b[32m     45\u001b[39m     experiment_name = \u001b[33mf\"exp_{agent_type.lower()}_{TRAIN_VERSION}\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m manager.train_agent(agent, env, config, experiment_name)\n",
      "\u001b[32m/mnt/d/Study/python/TradingAgentClassicRL/notebooks/../training/training_manager.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, agent, env, config, experiment_name, verbose)\u001b[39m\n\u001b[32m     59\u001b[39m \n\u001b[32m     60\u001b[39m             action = agent.select_action(state, training=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     61\u001b[39m \n\u001b[32m     62\u001b[39m             \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m done \u001b[38;5;28;01mand\u001b[39;00m steps < config.max_steps:\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m                 next_state, reward, terminated, truncated, info = env.step(action)\n\u001b[32m     64\u001b[39m                 done = terminated \u001b[38;5;28;01mor\u001b[39;00m truncated\n\u001b[32m     65\u001b[39m \n\u001b[32m     66\u001b[39m                 next_action = (\n",
      "\u001b[32m/mnt/d/Study/python/TradingAgentClassicRL/notebooks/../envs/trading_env.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m step(self, action: int) -> Tuple[np.ndarray, float, bool, bool, Dict[str, Any]]:\n\u001b[32m    179\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m self.action_space.contains(action)\n\u001b[32m    180\u001b[39m         prev_portfolio_value = self.portfolio_value\n\u001b[32m--> \u001b[39m\u001b[32m181\u001b[39m         current_price = float(self.df.iloc[self.current_step][\u001b[33m\"close\"\u001b[39m])\n\u001b[32m    182\u001b[39m         exit_reason = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    183\u001b[39m \n\u001b[32m    184\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m self.position == \u001b[32m0\u001b[39m:\n",
      "\u001b[32m~/miniconda3/envs/rl/lib/python3.11/site-packages/pandas/core/indexing.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   1187\u001b[39m             axis = self.axis \u001b[38;5;28;01mor\u001b[39;00m \u001b[32m0\u001b[39m\n\u001b[32m   1188\u001b[39m \n\u001b[32m   1189\u001b[39m             maybe_callable = com.apply_if_callable(key, self.obj)\n\u001b[32m   1190\u001b[39m             maybe_callable = self._check_deprecated_callable_usage(key, maybe_callable)\n\u001b[32m-> \u001b[39m\u001b[32m1191\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m self._getitem_axis(maybe_callable, axis=axis)\n",
      "\u001b[32m~/miniconda3/envs/rl/lib/python3.11/site-packages/pandas/core/indexing.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, key, axis)\u001b[39m\n\u001b[32m   1750\u001b[39m \n\u001b[32m   1751\u001b[39m             \u001b[38;5;66;03m# validate the location\u001b[39;00m\n\u001b[32m   1752\u001b[39m             self._validate_integer(key, axis)\n\u001b[32m   1753\u001b[39m \n\u001b[32m-> \u001b[39m\u001b[32m1754\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m self.obj._ixs(key, axis=axis)\n",
      "\u001b[32m~/miniconda3/envs/rl/lib/python3.11/site-packages/pandas/core/frame.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, i, axis)\u001b[39m\n\u001b[32m   4002\u001b[39m \n\u001b[32m   4003\u001b[39m             \u001b[38;5;66;03m# if we are a copy, mark as such\u001b[39;00m\n\u001b[32m   4004\u001b[39m             copy = isinstance(new_mgr.array, np.ndarray) \u001b[38;5;28;01mand\u001b[39;00m new_mgr.array.base \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4005\u001b[39m             result = self._constructor_sliced_from_mgr(new_mgr, axes=new_mgr.axes)\n\u001b[32m-> \u001b[39m\u001b[32m4006\u001b[39m             result._name = self.index[i]\n\u001b[32m   4007\u001b[39m             result = result.__finalize__(self)\n\u001b[32m   4008\u001b[39m             result._set_is_copy(self, copy=copy)\n\u001b[32m   4009\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[32m~/miniconda3/envs/rl/lib/python3.11/site-packages/pandas/core/generic.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, name, value)\u001b[39m\n\u001b[32m   6329\u001b[39m \n\u001b[32m   6330\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   6331\u001b[39m             object.__getattribute__(self, name)\n\u001b[32m   6332\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m object.__setattr__(self, name, value)\n\u001b[32m-> \u001b[39m\u001b[32m6333\u001b[39m         \u001b[38;5;28;01mexcept\u001b[39;00m AttributeError:\n\u001b[32m   6334\u001b[39m             \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m   6335\u001b[39m \n\u001b[32m   6336\u001b[39m         \u001b[38;5;66;03m# if this fails, go on to more involved attribute setting\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "results = run_training(\n",
    "    agent_type=\"QLearning\",\n",
    "    df=df,\n",
    "    project_root=project_root,\n",
    "    env_params=env_params,\n",
    "    train_params=train_params\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe6f361",
   "metadata": {},
   "source": [
    "### Обучение MonteCarloAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e346551c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params[\"max_steps\"] = MAX_STEPS // 2\n",
    "\n",
    "results = run_training(\n",
    "    agent_type=\"Monte_Carlo\",\n",
    "    df=df,\n",
    "    project_root=project_root,\n",
    "    env_params=env_params,\n",
    "    train_params=train_params\n",
    ")\n",
    "train_params[\"max_steps\"] = MAX_STEPS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20efa51f",
   "metadata": {},
   "source": [
    "### Обучение SarsaAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3131b50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = run_training(\n",
    "    agent_type=\"SARSA\",\n",
    "    df=df,\n",
    "    project_root=project_root,\n",
    "    env_params=env_params,\n",
    "    train_params=train_params\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a891c4d7",
   "metadata": {},
   "source": [
    "### Обучение SarsaLambdaAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd85913",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params_lambda = {\n",
    "    **train_params,\n",
    "    \"lambda_param\": 0.8,\n",
    "}\n",
    "\n",
    "results = run_training(\n",
    "    agent_type=\"SARSA_Lambda\",\n",
    "    df=df,\n",
    "    project_root=project_root,\n",
    "    env_params=env_params,\n",
    "    train_params=train_params_lambda\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93eb4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "experiment_dirs = [\n",
    "    f\"../training_data/logs/exp_qlearning_{TRAIN_VERSION}\",\n",
    "    f\"../training_data/logs/exp_monte_carlo_{TRAIN_VERSION}\",\n",
    "    f\"../training_data/logs/exp_sarsa_{TRAIN_VERSION}\",\n",
    "    f\"../training_data/logs/exp_sarsa_lambda_{TRAIN_VERSION}\",\n",
    "]\n",
    "\n",
    "all_data = []\n",
    "for exp_dir in experiment_dirs:\n",
    "    exp_dir = Path(exp_dir)\n",
    "    episodes_df = pd.read_csv(exp_dir / \"episodes.csv\")\n",
    "    with open(exp_dir / \"training_summary.json\") as f:\n",
    "        summary = json.load(f)\n",
    "    agent_name = summary[\"config\"][\"agent_name\"]\n",
    "    episodes_df[\"agent\"] = agent_name\n",
    "    all_data.append(episodes_df)\n",
    "\n",
    "\n",
    "df = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "for agent in df[\"agent\"].unique():\n",
    "    agent_data = df[df[\"agent\"] == agent]\n",
    "    smoothed = agent_data[\"reward\"].rolling(window=50, min_periods=1).mean()\n",
    "    plt.plot(agent_data[\"episode\"], smoothed, label=agent)\n",
    "\n",
    "plt.title(\"Сравнение средней награды (скользящее окно = 50)\")\n",
    "plt.xlabel(\"Эпизод\")\n",
    "plt.ylabel(\"Награда (сглаженная)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "for agent in df[\"agent\"].unique():\n",
    "    agent_data = df[df[\"agent\"] == agent]\n",
    "    plt.plot(agent_data[\"episode\"], agent_data[\"portfolio_value\"], label=agent)\n",
    "\n",
    "plt.title(\"Динамика стоимости портфеля\")\n",
    "plt.xlabel(\"Эпизод\")\n",
    "plt.ylabel(\"Portfolio Value ($)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "final_metrics = df.groupby(\"agent\").tail(1)[\n",
    "    [\"agent\", \"reward\", \"portfolio_value\", \"win_rate\", \"n_trades\", \"max_drawdown\"]\n",
    "]\n",
    "display(final_metrics.round(2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
