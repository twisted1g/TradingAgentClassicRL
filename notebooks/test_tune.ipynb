{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9a47178",
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "current_dir = Path.cwd()\n",
        "if current_dir.name == \"notebooks\":\n",
        "    project_root = current_dir.parent\n",
        "    sys.path.insert(0, str(project_root))\n",
        "else:\n",
        "    project_root = Path(os.getcwd())\n",
        "    sys.path.insert(0, str(project_root))\n",
        "\n",
        "import pandas as pd\n",
        "from training import TrainingManager, TrainingConfig\n",
        "\n",
        "TRAIN_VERSION = \"v3\"\n",
        "N_EPISODES_INITIAL = 5000   \n",
        "N_EPISODES_FINETUNE = 2000\n",
        "N_EPISODES_START_FINETUNE = N_EPISODES_INITIAL \n",
        "N_EPISODES_END_FINETUNE = N_EPISODES_INITIAL + N_EPISODES_FINETUNE\n",
        "\n",
        "\n",
        "data_path_2021 = project_root / \"data\" / \"data_1h_2021.csv\"\n",
        "data_path_2022 = project_root / \"data\" / \"data_1h_2022.csv\"\n",
        "data_path_2023 = project_root / \"data\" / \"data_1h_2023.csv\"\n",
        "\n",
        "if not data_path_2021.exists():\n",
        "    data_path_2021 = project_root.parent / \"data\" / \"data_1h_2021.csv\"\n",
        "    data_path_2022 = project_root.parent / \"data\" / \"data_1h_2022.csv\"\n",
        "    data_path_2023 = project_root.parent / \"data\" / \"data_1h_2023.csv\"\n",
        "\n",
        "df1 = pd.read_csv(data_path_2021, index_col=0, parse_dates=True, date_format=\"iso8601\")\n",
        "df2 = pd.read_csv(data_path_2022, index_col=0, parse_dates=True, date_format=\"iso8601\")\n",
        "df3 = pd.read_csv(data_path_2023, index_col=0, parse_dates=True, date_format=\"iso8601\") if data_path_2023.exists() else None\n",
        "\n",
        "\n",
        "\n",
        "df_train = pd.concat([df1, df2.iloc[:len(df2)//2]])\n",
        "\n",
        "\n",
        "# if df3 is not None:\n",
        "#     df_finetune = pd.concat([df2.iloc[len(df2)//2:], df3])\n",
        "# else:\n",
        "df_finetune = df2.iloc[len(df2)//2:]\n",
        "\n",
        "print(f\"Данные для дообучения: {len(df_finetune)} строк\")\n",
        "print(f\"Период: {df_finetune.index[0]} - {df_finetune.index[-1]}\")\n",
        "\n",
        "\n",
        "env_params = {\n",
        "    \"initial_balance\": 1000.0,\n",
        "    \"window_size\": 10,\n",
        "    \"commission\": 0.0001,\n",
        "    \"slippage\": 0.0001,\n",
        "    \"max_holding_time\": 48,\n",
        "    \"holding_threshold\": 12, \n",
        "    \"max_drawdown_threshold\": 0.05,\n",
        "    \"lambda_drawdown\": 0.5,\n",
        "    \"lambda_hold\": 0.1,\n",
        "    \"reward_scaling\": 1.0, \n",
        "    \"max_steps\": 1000\n",
        "}\n",
        "\n",
        "\n",
        "base_train_params = {\n",
        "    \"n_episodes\": N_EPISODES_INITIAL,\n",
        "    \"n_episodes_start\": 0,\n",
        "    \"max_steps\": 1000,\n",
        "    \"eval_frequency\": 100,\n",
        "    \"save_frequency\": 500,\n",
        "    \"patience\": 200,\n",
        "    \"initial_balance\": 1000.0,\n",
        "    **{k: env_params[k] for k in env_params if k != \"max_steps\"} \n",
        "}\n",
        "\n",
        "\n",
        "finetune_train_params = {\n",
        "    \"n_episodes\": N_EPISODES_END_FINETUNE, # 4500\n",
        "    \"n_episodes_start\": N_EPISODES_START_FINETUNE, # 3000\n",
        "    \"max_steps\": 1000,\n",
        "    \"eval_frequency\": 50, # Увеличиваем частоту оценки\n",
        "    \"save_frequency\": 500,\n",
        "    \"patience\": 200,\n",
        "    \"initial_balance\": 1000.0,\n",
        "    **{k: env_params[k] for k in env_params if k != \"max_steps\"}\n",
        "}\n",
        "\n",
        "\n",
        "agents_config = {\n",
        "    \"QLearning\": {\n",
        "        \"learning_rate\": 0.05, \"discount_factor\": 0.99,\n",
        "        \"epsilon_start\": 1.0, \"epsilon_end\": 0.01, \"epsilon_decay\": 0.998,\n",
        "    },\n",
        "    \"SARSA\": {\n",
        "        \"learning_rate\": 0.05, \"discount_factor\": 0.99,\n",
        "        \"epsilon_start\": 1.0, \"epsilon_end\": 0.01, \"epsilon_decay\": 0.998,\n",
        "    },\n",
        "    \"SARSA_Lambda\": {\n",
        "        \"learning_rate\": 0.02, \"discount_factor\": 0.99,\n",
        "        \"epsilon_start\": 1.0, \"epsilon_end\": 0.01, \"epsilon_decay\": 0.998,\n",
        "        \"trace_decay\": 0.9,\n",
        "    },\n",
        "    \"Monte_Carlo\": {\n",
        "        \"learning_rate\": 0.02, \"discount_factor\": 0.99,\n",
        "        \"epsilon_start\": 1.0, \"epsilon_end\": 0.05,  \"epsilon_decay\": 0.999, \n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "def _get_agent_instance(agent_type: str, hyperparams: dict):\n",
        "    if agent_type == \"SARSA\":\n",
        "        from agents.classical.sarsa_agent import SarsaAgent\n",
        "        agent = SarsaAgent()\n",
        "    elif agent_type == \"SARSA_Lambda\":\n",
        "        from agents.classical.sarsa_lambda_agent import SarsaLambdaAgent\n",
        "        agent = SarsaLambdaAgent()\n",
        "    elif agent_type == \"QLearning\":\n",
        "        from agents.classical.qlearning_agent import QLearningAgent\n",
        "        agent = QLearningAgent()\n",
        "    elif agent_type == \"Monte_Carlo\":\n",
        "        from agents.classical.monte_carlo_agent import MonteCarloAgent\n",
        "        agent = MonteCarloAgent()\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown agent type: {agent_type}\")\n",
        "        \n",
        "    for key, value in hyperparams.items():\n",
        "        if hasattr(agent, key):\n",
        "             setattr(agent, key, value)\n",
        "    return agent\n",
        "\n",
        "\n",
        "def run_finetuning(\n",
        "    agent_type: str,\n",
        "    df,\n",
        "    project_root: str,\n",
        "    env_params: dict,\n",
        "    train_params: dict,\n",
        "):\n",
        "    hyperparams = agents_config.get(agent_type, {})\n",
        "    \n",
        "    experiment_name_base = f\"exp_{agent_type.lower()}_{TRAIN_VERSION}\"\n",
        "    experiment_name_finetune = f\"{experiment_name_base}_finetune\"\n",
        "    \n",
        "    checkpoint_path = os.path.join(\n",
        "        project_root, \"training_data\", \"checkpoints\", experiment_name_base, \"final_agent.pkl\",\n",
        "    )\n",
        "\n",
        "    if not Path(checkpoint_path).exists():\n",
        "        print(f\"!!! ОШИБКА: Файл чекпоинта не найден: {checkpoint_path}\")\n",
        "        raise FileNotFoundError(\"Сначала запустите и завершите ПЕРВИЧНОЕ ОБУЧЕНИЕ.\")\n",
        "\n",
        "    agent = _get_agent_instance(agent_type, hyperparams) \n",
        "\n",
        "    continue_config = TrainingConfig(\n",
        "        agent_name=f\"{agent_type}_{TRAIN_VERSION}_finetune\",\n",
        "        agent_type=agent_type,\n",
        "        **train_params,\n",
        "    )\n",
        "\n",
        "    manager = TrainingManager(\n",
        "        base_log_dir=os.path.join(project_root, \"training_data/logs\"),\n",
        "        base_checkpoint_dir=os.path.join(project_root, \"training_data/checkpoints\"),\n",
        "    )\n",
        "    \n",
        "    print(f\"\\n=======================================================\")\n",
        "    print(f\" ♻️ НАЧАЛО ДООБУЧЕНИЯ: {agent_type} (Ep: {train_params['n_episodes_start']} -> {train_params['n_episodes']})\")\n",
        "    print(f\"=======================================================\")\n",
        "    \n",
        "    return manager.continue_training(\n",
        "        agent=agent,\n",
        "        df=df,\n",
        "        config=continue_config,\n",
        "        checkpoint_path=checkpoint_path,\n",
        "        experiment_name=experiment_name_finetune,\n",
        "        verbose=True,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23bd2ce8",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "for agent_name in agents_config.keys():\n",
        "    try:\n",
        "        run_finetuning(\n",
        "            agent_type=agent_name,\n",
        "            df=df_finetune,\n",
        "            project_root=project_root,\n",
        "            env_params=env_params,\n",
        "            train_params=finetune_train_params,\n",
        "        )\n",
        "    except FileNotFoundError:\n",
        "        print(f\"--- Пропуск дообучения {agent_name}: Не найден базовый чекпоинт. ---\")\n",
        "        continue\n",
        "\n",
        "    \n",
        "# run_finetuning(\n",
        "#     agent_type=\"QLearning\",\n",
        "#     df=df_finetune,\n",
        "#     project_root=project_root,\n",
        "#     env_params=env_params,\n",
        "#     train_params=finetune_train_params,\n",
        "# )\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
