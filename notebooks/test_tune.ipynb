{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9a47178",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =========================================================================\n",
        "# JUPYTER CELL 1: –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø, –§–£–ù–ö–¶–ò–ò –ò –ü–ê–†–ê–ú–ï–¢–†–´\n",
        "# –°–æ–¥–µ—Ä–∂–∏—Ç –≤—Å—é –ª–æ–≥–∏–∫—É –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏ –¥–æ–æ–±—É—á–µ–Ω–∏—è\n",
        "# =========================================================================\n",
        "\n",
        "# =========================================================================\n",
        "# –ë–õ–û–ö–ù–û–¢ –î–õ–Ø –î–û–û–ë–£–ß–ï–ù–ò–Ø (FINE-TUNING) –ê–ì–ï–ù–¢–û–í\n",
        "# =========================================================================\n",
        "\n",
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# –ü—Ä–∞–≤–∏–ª—å–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –ø—É—Ç–µ–π\n",
        "current_dir = Path.cwd()\n",
        "if current_dir.name == \"notebooks\":\n",
        "    project_root = current_dir.parent\n",
        "    sys.path.insert(0, str(project_root))\n",
        "else:\n",
        "    project_root = Path(os.getcwd())\n",
        "    sys.path.insert(0, str(project_root))\n",
        "\n",
        "import pandas as pd\n",
        "from training import TrainingManager, TrainingConfig\n",
        "\n",
        "# --- 1. –ì–õ–û–ë–ê–õ–¨–ù–´–ï –ü–ê–†–ê–ú–ï–¢–†–´ –í–ï–†–°–ò–ò –ò –≠–ü–ò–ó–û–î–û–í ---\n",
        "TRAIN_VERSION = \"v3\"  # –î–æ–ª–∂–Ω–æ —Å–æ–≤–ø–∞–¥–∞—Ç—å —Å –≤–µ—Ä—Å–∏–µ–π –≤ test_train.ipynb\n",
        "N_EPISODES_INITIAL = 5000   \n",
        "N_EPISODES_FINETUNE = 2000  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–∏–∑–æ–¥–æ–≤ –Ω–∞ –≤—Ç–æ—Ä–æ–º —ç—Ç–∞–ø–µ\n",
        "N_EPISODES_START_FINETUNE = N_EPISODES_INITIAL \n",
        "N_EPISODES_END_FINETUNE = N_EPISODES_INITIAL + N_EPISODES_FINETUNE\n",
        "\n",
        "\n",
        "# --- 2. –ó–ê–ì–†–£–ó–ö–ê –ò –†–ê–ó–î–ï–õ–ï–ù–ò–ï –î–ê–ù–ù–´–• ---\n",
        "data_path_2021 = project_root / \"data\" / \"data_1h_2021.csv\"\n",
        "data_path_2022 = project_root / \"data\" / \"data_1h_2022.csv\"\n",
        "data_path_2023 = project_root / \"data\" / \"data_1h_2023.csv\"\n",
        "\n",
        "if not data_path_2021.exists():\n",
        "    data_path_2021 = project_root.parent / \"data\" / \"data_1h_2021.csv\"\n",
        "    data_path_2022 = project_root.parent / \"data\" / \"data_1h_2022.csv\"\n",
        "    data_path_2023 = project_root.parent / \"data\" / \"data_1h_2023.csv\"\n",
        "\n",
        "df1 = pd.read_csv(data_path_2021, index_col=0, parse_dates=True, date_format=\"iso8601\")\n",
        "df2 = pd.read_csv(data_path_2022, index_col=0, parse_dates=True, date_format=\"iso8601\")\n",
        "df3 = pd.read_csv(data_path_2023, index_col=0, parse_dates=True, date_format=\"iso8601\") if data_path_2023.exists() else None\n",
        "\n",
        "\n",
        "# –î–∞–Ω–Ω—ã–µ –¥–ª—è –ø–µ—Ä–≤–∏—á–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è (2021 + 1/2 2022)\n",
        "df_train = pd.concat([df1, df2.iloc[:len(df2)//2]])\n",
        "\n",
        "# –î–∞–Ω–Ω—ã–µ –¥–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è (1/2 2022 + 2023)\n",
        "if df3 is not None:\n",
        "    df_finetune = pd.concat([df2.iloc[len(df2)//2:], df3])\n",
        "else:\n",
        "    df_finetune = df2.iloc[len(df2)//2:]\n",
        "\n",
        "print(f\"–î–∞–Ω–Ω—ã–µ –¥–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è: {len(df_finetune)} —Å—Ç—Ä–æ–∫\")\n",
        "print(f\"–ü–µ—Ä–∏–æ–¥: {df_finetune.index[0]} - {df_finetune.index[-1]}\")\n",
        "\n",
        "\n",
        "# --- 3. –ü–ê–†–ê–ú–ï–¢–†–´ –°–†–ï–î–´ (ENV PARAMS) ---\n",
        "env_params = {\n",
        "    \"initial_balance\": 1000.0,\n",
        "    \"window_size\": 10,\n",
        "    \"commission\": 0.0001,\n",
        "    \"slippage\": 0.0001,\n",
        "    \"max_holding_time\": 48,\n",
        "    \"holding_threshold\": 12, \n",
        "    \"max_drawdown_threshold\": 0.05,\n",
        "    \"lambda_drawdown\": 0.5,\n",
        "    \"lambda_hold\": 0.1,\n",
        "    \"reward_scaling\": 1.0, \n",
        "    \"max_steps\": 1000\n",
        "}\n",
        "\n",
        "# --- 4. –ü–ê–†–ê–ú–ï–¢–†–´ –û–ë–£–ß–ï–ù–ò–Ø (TRAIN PARAMS) ---\n",
        "\n",
        "# 4.1. –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ü–µ—Ä–≤–∏—á–Ω–æ–≥–æ –û–±—É—á–µ–Ω–∏—è\n",
        "base_train_params = {\n",
        "    \"n_episodes\": N_EPISODES_INITIAL,\n",
        "    \"n_episodes_start\": 0,\n",
        "    \"max_steps\": 1000,\n",
        "    \"eval_frequency\": 100,\n",
        "    \"save_frequency\": 500,\n",
        "    \"patience\": 200,\n",
        "    \"initial_balance\": 1000.0,\n",
        "    **{k: env_params[k] for k in env_params if k != \"max_steps\"} \n",
        "}\n",
        "\n",
        "# 4.2. –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –î–æ–æ–±—É—á–µ–Ω–∏—è\n",
        "finetune_train_params = {\n",
        "    \"n_episodes\": N_EPISODES_END_FINETUNE, # 4500\n",
        "    \"n_episodes_start\": N_EPISODES_START_FINETUNE, # 3000\n",
        "    \"max_steps\": 1000,\n",
        "    \"eval_frequency\": 50, # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —á–∞—Å—Ç–æ—Ç—É –æ—Ü–µ–Ω–∫–∏\n",
        "    \"save_frequency\": 500,\n",
        "    \"patience\": 200,\n",
        "    \"initial_balance\": 1000.0,\n",
        "    **{k: env_params[k] for k in env_params if k != \"max_steps\"}\n",
        "}\n",
        "\n",
        "# 4.3. –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –ê–≥–µ–Ω—Ç–æ–≤ (–ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã)\n",
        "agents_config = {\n",
        "    \"QLearning\": {\n",
        "        \"learning_rate\": 0.05, \"discount_factor\": 0.99,\n",
        "        \"epsilon_start\": 1.0, \"epsilon_end\": 0.01, \"epsilon_decay\": 0.998,\n",
        "    },\n",
        "    \"SARSA\": {\n",
        "        \"learning_rate\": 0.05, \"discount_factor\": 0.99,\n",
        "        \"epsilon_start\": 1.0, \"epsilon_end\": 0.01, \"epsilon_decay\": 0.998,\n",
        "    },\n",
        "    \"SARSA_Lambda\": {\n",
        "        \"learning_rate\": 0.02, \"discount_factor\": 0.99,\n",
        "        \"epsilon_start\": 1.0, \"epsilon_end\": 0.01, \"epsilon_decay\": 0.998,\n",
        "        \"trace_decay\": 0.9,\n",
        "    },\n",
        "    \"Monte_Carlo\": {\n",
        "        \"learning_rate\": 0.02, \"discount_factor\": 0.99,\n",
        "        \"epsilon_start\": 1.0, \"epsilon_end\": 0.05,  \"epsilon_decay\": 0.999, \n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "# --- 5. –§–£–ù–ö–¶–ò–ò ---\n",
        "\n",
        "def _get_agent_instance(agent_type: str, hyperparams: dict):\n",
        "    \"\"\"–°–æ–∑–¥–∞–µ—Ç —ç–∫–∑–µ–º–ø–ª—è—Ä –∞–≥–µ–Ω—Ç–∞ –∏ –ø—Ä–∏—Å–≤–∞–∏–≤–∞–µ—Ç –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã.\"\"\"\n",
        "    if agent_type == \"SARSA\":\n",
        "        from agents.classical.sarsa_agent import SarsaAgent\n",
        "        agent = SarsaAgent()\n",
        "    elif agent_type == \"SARSA_Lambda\":\n",
        "        from agents.classical.sarsa_lambda_agent import SarsaLambdaAgent\n",
        "        agent = SarsaLambdaAgent()\n",
        "    elif agent_type == \"QLearning\":\n",
        "        from agents.classical.qlearning_agent import QLearningAgent\n",
        "        agent = QLearningAgent()\n",
        "    elif agent_type == \"Monte_Carlo\":\n",
        "        from agents.classical.monte_carlo_agent import MonteCarloAgent\n",
        "        agent = MonteCarloAgent()\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown agent type: {agent_type}\")\n",
        "        \n",
        "    for key, value in hyperparams.items():\n",
        "        if hasattr(agent, key):\n",
        "             setattr(agent, key, value)\n",
        "    return agent\n",
        "\n",
        "\n",
        "def run_training(\n",
        "    agent_type: str,\n",
        "    df,\n",
        "    project_root: str,\n",
        "    env_params: dict,\n",
        "    train_params: dict,\n",
        "):\n",
        "    \"\"\"–ó–∞–ø—É—Å–∫–∞–µ—Ç –ø–µ—Ä–≤–∏—á–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è –æ–¥–Ω–æ–≥–æ –∞–≥–µ–Ω—Ç–∞.\"\"\"\n",
        "    hyperparams = agents_config.get(agent_type, {})\n",
        "    agent = _get_agent_instance(agent_type, hyperparams)\n",
        "\n",
        "    config = TrainingConfig(\n",
        "        agent_name=f\"{agent_type}_{TRAIN_VERSION}\",\n",
        "        agent_type=agent_type,\n",
        "        **train_params,\n",
        "    )\n",
        "\n",
        "    manager = TrainingManager(\n",
        "        base_log_dir=os.path.join(project_root, \"training_data/logs\"),\n",
        "        base_checkpoint_dir=os.path.join(project_root, \"training_data/checkpoints\"),\n",
        "    )\n",
        "\n",
        "    experiment_name = f\"exp_{agent_type.lower()}_{TRAIN_VERSION}\"\n",
        "    print(f\"\\n=======================================================\")\n",
        "    print(f\" üöÄ –ù–ê–ß–ê–õ–û –ü–ï–†–í–ò–ß–ù–û–ì–û –û–ë–£–ß–ï–ù–ò–Ø: {agent_type} (Ep: 0 -> {train_params['n_episodes']})\")\n",
        "    print(f\"=======================================================\")\n",
        "    return manager.train_agent(agent, df, config, experiment_name)\n",
        "\n",
        "\n",
        "def run_finetuning(\n",
        "    agent_type: str,\n",
        "    df,\n",
        "    project_root: str,\n",
        "    env_params: dict,\n",
        "    train_params: dict, # finetune_train_params\n",
        "):\n",
        "    \"\"\"–ó–∞–ø—É—Å–∫–∞–µ—Ç –¥–æ–æ–±—É—á–µ–Ω–∏–µ (Fine-tuning) —Å –∑–∞–≥—Ä—É–∑–∫–æ–π —á–µ–∫–ø–æ–∏–Ω—Ç–∞.\"\"\"\n",
        "\n",
        "    hyperparams = agents_config.get(agent_type, {})\n",
        "    \n",
        "    # 1. –ü—É—Ç–∏ –∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–º—É –∞–≥–µ–Ω—Ç—É (–∏–∑ exp_{agent}_v2)\n",
        "    experiment_name_base = f\"exp_{agent_type.lower()}_{TRAIN_VERSION}\"\n",
        "    experiment_name_finetune = f\"{experiment_name_base}_finetune\"\n",
        "    \n",
        "    checkpoint_path = os.path.join(\n",
        "        project_root, \"training_data\", \"checkpoints\", experiment_name_base, \"final_agent.pkl\",\n",
        "    )\n",
        "\n",
        "    if not Path(checkpoint_path).exists():\n",
        "        print(f\"!!! –û–®–ò–ë–ö–ê: –§–∞–π–ª —á–µ–∫–ø–æ–∏–Ω—Ç–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω: {checkpoint_path}\")\n",
        "        raise FileNotFoundError(\"–°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ –∏ –∑–∞–≤–µ—Ä—à–∏—Ç–µ –ü–ï–†–í–ò–ß–ù–û–ï –û–ë–£–ß–ï–ù–ò–ï.\")\n",
        "\n",
        "    # 2. –°–æ–∑–¥–∞–Ω–∏–µ –∞–≥–µ–Ω—Ç–∞\n",
        "    agent = _get_agent_instance(agent_type, hyperparams) \n",
        "\n",
        "    # 3. –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–æ–æ–±—É—á–µ–Ω–∏—è (—É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ –Ω—É–º–µ—Ä–∞—Ü–∏–∏ —ç–ø–∏–∑–æ–¥–æ–≤)\n",
        "    continue_config = TrainingConfig(\n",
        "        agent_name=f\"{agent_type}_{TRAIN_VERSION}_finetune\",\n",
        "        agent_type=agent_type,\n",
        "        **train_params,\n",
        "    )\n",
        "\n",
        "    # 4. –ú–µ–Ω–µ–¥–∂–µ—Ä –∏ –∑–∞–ø—É—Å–∫\n",
        "    manager = TrainingManager(\n",
        "        base_log_dir=os.path.join(project_root, \"training_data/logs\"),\n",
        "        base_checkpoint_dir=os.path.join(project_root, \"training_data/checkpoints\"),\n",
        "    )\n",
        "    \n",
        "    print(f\"\\n=======================================================\")\n",
        "    print(f\" ‚ôªÔ∏è –ù–ê–ß–ê–õ–û –î–û–û–ë–£–ß–ï–ù–ò–Ø: {agent_type} (Ep: {train_params['n_episodes_start']} -> {train_params['n_episodes']})\")\n",
        "    print(f\"=======================================================\")\n",
        "    \n",
        "    return manager.continue_training(\n",
        "        agent=agent,\n",
        "        df=df,\n",
        "        config=continue_config,\n",
        "        checkpoint_path=checkpoint_path,\n",
        "        experiment_name=experiment_name_finetune,\n",
        "        verbose=True,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23bd2ce8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =========================================================================\n",
        "# JUPYTER CELL 2: –ó–ê–ü–£–°–ö –î–û–û–ë–£–ß–ï–ù–ò–Ø (FINE-TUNING)\n",
        "# –ó–∞–ø—É—Å–∫–∞—Ç—å –ø–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –≤—Å–µ—Ö —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ–∫ –∏–∑ –ß–∞—Å—Ç–∏ 1\n",
        "# =========================================================================\n",
        "\n",
        "# –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –Ø—á–µ–π–∫–∞ 1 –≤—ã–ø–æ–ª–Ω–µ–Ω–∞!\n",
        "\n",
        "for agent_name in agents_config.keys():\n",
        "    try:\n",
        "        run_finetuning(\n",
        "            agent_type=agent_name,\n",
        "            df=df_finetune,\n",
        "            project_root=project_root,\n",
        "            env_params=env_params,\n",
        "            train_params=finetune_train_params,\n",
        "        )\n",
        "    except FileNotFoundError:\n",
        "        print(f\"--- –ü—Ä–æ–ø—É—Å–∫ –¥–æ–æ–±—É—á–µ–Ω–∏—è {agent_name}: –ù–µ –Ω–∞–π–¥–µ–Ω –±–∞–∑–æ–≤—ã–π —á–µ–∫–ø–æ–∏–Ω—Ç. ---\")\n",
        "        continue\n",
        "# –ß—Ç–æ–±—ã –∑–∞–ø—É—Å—Ç–∏—Ç—å —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω –∞–≥–µ–Ω—Ç, –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ:\n",
        "# run_finetuning(\n",
        "#     agent_type=\"QLearning\",\n",
        "#     df=df_finetune,\n",
        "#     project_root=project_root,\n",
        "#     env_params=env_params,\n",
        "#     train_params=finetune_train_params,\n",
        "# )\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
