{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f200b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from envs.trading_env import MyTradingEnv\n",
    "from agents.classical.qlearning_agent import QLearningAgent\n",
    "from agents.classical.sarsa_agent import SarsaAgent\n",
    "from agents.classical.sarsa_lambda_agent import SarsaLambdaAgent\n",
    "from agents.classical.monte_carlo_agent import MonteCarloAgent\n",
    "from training.training_manager import TrainingManager\n",
    "from training.dataclasses import TrainingConfig\n",
    "\n",
    "def retrain_with_fixes():\n",
    "    \"\"\"Переобучение агентов с исправлениями\"\"\"\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"ПЕРЕОБУЧЕНИЕ АГЕНТОВ С ИСПРАВЛЕНИЯМИ\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Загружаем данные\n",
    "    data_path = \"../data/data_1h_2023.csv\"\n",
    "    df = pd.read_csv(data_path, index_col=0, parse_dates=True, date_format=\"iso8601\")\n",
    "    print(f\"Загружено данных: {len(df)} строк\")\n",
    "    \n",
    "    # Создаем окружение с улучшенными настройками\n",
    "    def create_env():\n",
    "        return MyTradingEnv(\n",
    "            df=df.copy(),\n",
    "            initial_balance=1000,\n",
    "            window_size=10,\n",
    "            commission=0.001,  # 0.1% комиссия\n",
    "            slippage=0.0005,   # 0.05% проскальзывание\n",
    "            reward_scaling=0.01,  # Уменьшаем масштаб наград\n",
    "            lambda_drawdown=0.3,  # Увеличиваем штраф за просадку\n",
    "            lambda_hold=0.05      # Уменьшаем штраф за удержание\n",
    "        )\n",
    "    \n",
    "    # Конфигурации для каждого агента\n",
    "    configs = {\n",
    "        \"Q-Learning\": TrainingConfig(\n",
    "            agent_name=\"Q-Learning_v2\",\n",
    "            agent_type=\"Q-Learning\",\n",
    "            n_episodes=3000,\n",
    "            learning_rate=0.05,\n",
    "            discount_factor=0.97,\n",
    "            epsilon_start=0.3,\n",
    "            epsilon_end=0.01,\n",
    "            epsilon_decay=0.998,\n",
    "            eval_frequency=100,\n",
    "            save_frequency=500\n",
    "        ),\n",
    "        \"SARSA\": TrainingConfig(\n",
    "            agent_name=\"SARSA_v2\",\n",
    "            agent_type=\"SARSA\",\n",
    "            n_episodes=4000,  # Больше эпизодов для SARSA\n",
    "            learning_rate=0.03,\n",
    "            discount_factor=0.95,  # Меньше дисконт\n",
    "            epsilon_start=0.2,\n",
    "            epsilon_end=0.005,\n",
    "            epsilon_decay=0.999,\n",
    "            eval_frequency=100,\n",
    "            save_frequency=500\n",
    "        ),\n",
    "        \"SARSA-λ\": TrainingConfig(\n",
    "            agent_name=\"SARSA-λ_v2\",\n",
    "            agent_type=\"SARSA-λ\",\n",
    "            n_episodes=3500,\n",
    "            learning_rate=0.02,\n",
    "            discount_factor=0.96,\n",
    "            epsilon_start=0.25,\n",
    "            epsilon_end=0.01,\n",
    "            epsilon_decay=0.997,\n",
    "            lambda_param=0.8,  # Добавляем lambda\n",
    "            eval_frequency=100,\n",
    "            save_frequency=500\n",
    "        ),\n",
    "        \"Monte Carlo\": TrainingConfig(\n",
    "            agent_name=\"Monte Carlo_v2\",\n",
    "            agent_type=\"Monte Carlo\",\n",
    "            n_episodes=2000,  # Меньше эпизодов для MC\n",
    "            learning_rate=0.01,  # Меньше learning rate\n",
    "            discount_factor=0.90,  # Меньше дисконт\n",
    "            epsilon_start=0.1,  # Меньше случайности\n",
    "            epsilon_end=0.001,\n",
    "            epsilon_decay=0.995,\n",
    "            eval_frequency=50,  # Чаще оценка\n",
    "            save_frequency=250\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    agents = {\n",
    "        \"Q-Learning\": QLearningAgent,\n",
    "        \"SARSA\": SarsaAgent,\n",
    "        \"SARSA-λ\": SarsaLambdaAgent,\n",
    "        \"Monte Carlo\": MonteCarloAgent\n",
    "    }\n",
    "    \n",
    "    manager = TrainingManager()\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for name, agent_class in agents.items():\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"ОБУЧЕНИЕ: {name}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        try:\n",
    "            # Создаем агента с улучшенными параметрами\n",
    "            if name == \"SARSA-λ\":\n",
    "                agent = agent_class(lambda_param=0.8)\n",
    "            else:\n",
    "                agent = agent_class()\n",
    "            \n",
    "            # Настраиваем параметры\n",
    "            config = configs[name]\n",
    "            \n",
    "            # Переопределяем некоторые параметры\n",
    "            agent.learning_rate = config.learning_rate\n",
    "            agent.discount_factor = config.discount_factor\n",
    "            agent.epsilon = config.epsilon_start\n",
    "            agent.epsilon_start = config.epsilon_start\n",
    "            agent.epsilon_end = config.epsilon_end\n",
    "            agent.epsilon_decay = config.epsilon_decay\n",
    "            \n",
    "            # Создаем новое окружение для обучения\n",
    "            env = create_env()\n",
    "            \n",
    "            # Обучаем\n",
    "            result = manager.train_agent(\n",
    "                agent=agent,\n",
    "                env=env,\n",
    "                config=config,\n",
    "                experiment_name=f\"exp_{name.lower().replace('-', '').replace(' ', '')}_v2\",\n",
    "                verbose=True\n",
    "            )\n",
    "            \n",
    "            results[name] = result\n",
    "            \n",
    "            print(f\"\\n✅ {name} обучен успешно!\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Ошибка обучения {name}: {e}\")\n",
    "    \n",
    "    # Сравнение результатов\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"СРАВНЕНИЕ РЕЗУЛЬТАТОВ ПЕРЕОБУЧЕНИЯ\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    comparison = []\n",
    "    for name, result in results.items():\n",
    "        if result:\n",
    "            # Тестируем обученного агента\n",
    "            env_test = create_env()\n",
    "            \n",
    "            # Создаем тестового агента\n",
    "            agent = agent_class()\n",
    "            agent.load(result['final_agent_path'])\n",
    "            \n",
    "            # Оценка\n",
    "            state, _ = env_test.reset()\n",
    "            done = False\n",
    "            \n",
    "            while not done:\n",
    "                action = agent.select_action(state, training=False)\n",
    "                next_state, reward, done, _, _ = env_test.step(action)\n",
    "                state = next_state\n",
    "            \n",
    "            metrics = env_test.get_metrics()\n",
    "            \n",
    "            comparison.append({\n",
    "                'Агент': name,\n",
    "                'Конечный баланс': env_test._portfolio_value,\n",
    "                'Доходность %': (env_test._portfolio_value / 1000 - 1) * 100,\n",
    "                'Сделок': metrics.get('total_trades', 0),\n",
    "                'Win Rate %': metrics.get('win_rate', 0),\n",
    "                'Средний PnL': metrics.get('avg_pnl', 0)\n",
    "            })\n",
    "    \n",
    "    if comparison:\n",
    "        df_comp = pd.DataFrame(comparison)\n",
    "        df_comp = df_comp.sort_values('Конечный баланс', ascending=False)\n",
    "        \n",
    "        print(\"\\nРезультаты:\")\n",
    "        print(df_comp.to_string(index=False, float_format=lambda x: f\"{x:.2f}\"))\n",
    "        \n",
    "        # Сохраняем\n",
    "        df_comp.to_csv(\"retraining_comparison.csv\", index=False)\n",
    "        print(f\"\\n✅ Результаты сохранены в retraining_comparison.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    retrain_with_fixes()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
