{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d064b844",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from envs.trading_env import MyTradingEnv\n",
    "\n",
    "\n",
    "INITIAL_BALANCE = 1000.0\n",
    "WINDOW_SIZE = 10\n",
    "COMMISSION = 0.0001\n",
    "SLIPPAGE = 0.0005\n",
    "MAX_HOLDING_TIME = 60 * 24\n",
    "HOLDING_THRESHOLD = 24\n",
    "MAX_DRAWDOWN_THRESHOLD = 0.05\n",
    "LAMBDA_DRAWDOWN = 0.1\n",
    "LAMBDA_HOLD = 0.01\n",
    "REWARD_SCALING=10.0\n",
    "MAX_STEPS=None\n",
    "\n",
    "TRAIN_VERSION = \"v2\"\n",
    "\n",
    "df1 = pd.read_csv(\"../data/data_1h_2021.csv\", index_col=0, parse_dates=True)\n",
    "df2 = pd.read_csv(\"../data/data_1h_2022.csv\", index_col=0, parse_dates=True)\n",
    "df3 = pd.read_csv(\"../data/data_1h_2023.csv\", index_col=0, parse_dates=True)\n",
    "df4 = pd.read_csv(\"../data/data_1h_2024.csv\", index_col=0, parse_dates=True)\n",
    "\n",
    "train_data = pd.concat([df1])\n",
    "\n",
    "df_full = train_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a99cd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "class QLearningLoaded:\n",
    "    def __init__(self, data):\n",
    "        self.q_table = data[\"q_table\"]\n",
    "        self.n_actions = data[\"n_actions\"]\n",
    "        self.epsilon = 0.0\n",
    "\n",
    "    def select_action(self, state, training=False):\n",
    "        key = tuple(state)\n",
    "        q = self.q_table.get(key, np.zeros(self.n_actions))\n",
    "        return int(np.argmax(q))\n",
    "\n",
    "\n",
    "class SARSALoaded:\n",
    "    def __init__(self, data):\n",
    "        self.q_table = data[\"q_table\"]\n",
    "        self.n_actions = data[\"n_actions\"]\n",
    "        self.epsilon = 0.0\n",
    "\n",
    "    def select_action(self, state, training=False):\n",
    "        key = tuple(state)\n",
    "        q = self.q_table.get(key, np.zeros(self.n_actions))\n",
    "        return int(np.argmax(q))\n",
    "\n",
    "\n",
    "class SARSALambdaLoaded:\n",
    "    def __init__(self, data):\n",
    "        self.q_table = data[\"q_table\"]\n",
    "        self.n_actions = data[\"n_actions\"]\n",
    "        self.lambda_param = data[\"lambda_param\"]\n",
    "        self.replace_traces = data[\"replace_traces\"]\n",
    "        self.epsilon = 0.0\n",
    "\n",
    "    def select_action(self, state, training=False):\n",
    "        key = tuple(state)\n",
    "        q = self.q_table.get(key, np.zeros(self.n_actions))\n",
    "        return int(np.argmax(q))\n",
    "\n",
    "\n",
    "class MonteCarloLoaded:\n",
    "    def __init__(self, data):\n",
    "        self.q_table = data.get(\"q_table\", {})\n",
    "        self.policy = data.get(\"policy\", {})\n",
    "        self.n_actions = data.get(\"n_actions\", 3)\n",
    "\n",
    "    def select_action(self, state, training=False):\n",
    "        key = tuple(state)\n",
    "        if key in self.policy:\n",
    "            return int(np.argmax(self.policy[key]))\n",
    "        if key in self.q_table:\n",
    "            return int(np.argmax(self.q_table[key]))\n",
    "        return np.random.randint(self.n_actions)\n",
    "\n",
    "\n",
    "def load_agent(path):\n",
    "    with open(path, \"rb\") as f:\n",
    "        agent_instance = pickle.load(f)\n",
    "\n",
    "    name = agent_instance.name.lower()\n",
    "\n",
    "    loaded_data = {\n",
    "        \"q_table\": agent_instance.q_table,\n",
    "        \"n_actions\": agent_instance.n_actions,\n",
    "        \"name\": agent_instance.name,\n",
    "    }\n",
    "\n",
    "    if \"lambda\" in name:\n",
    "        loaded_data[\"lambda_param\"] = getattr(agent_instance, 'lambda_param', None)\n",
    "        loaded_data[\"replace_traces\"] = getattr(agent_instance, 'replace_traces', None)\n",
    "        return SARSALambdaLoaded(loaded_data)\n",
    "        \n",
    "    if \"sarsa\" in name:\n",
    "        return SARSALoaded(loaded_data)\n",
    "        \n",
    "    if \"q\" in name:\n",
    "        return QLearningLoaded(loaded_data)\n",
    "        \n",
    "    if \"monte\" in name:\n",
    "        loaded_data[\"policy\"] = getattr(agent_instance, 'policy', {}) \n",
    "        return MonteCarloLoaded(loaded_data)\n",
    "\n",
    "    raise ValueError(f\"Неизвестный агент: {agent_instance.name}\")\n",
    "\n",
    "\n",
    "class ConsistentAgentComparator:\n",
    "    def __init__(self, seed=42):\n",
    "        self.seed = seed\n",
    "        np.random.seed(seed)\n",
    "        self.test_start = None\n",
    "    \n",
    "    def run_episode(self, env, agent, record_values=False):\n",
    "        state, info = env.reset(seed=self.seed)\n",
    "        done = False\n",
    "        values = []\n",
    "        \n",
    "        if self.test_start is None:\n",
    "            self.test_start = env.current_step\n",
    "        \n",
    "        done = False\n",
    "        step = 0\n",
    "\n",
    "        while not done:\n",
    "            action = agent.select_action(state, training=False)\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "            state = next_state\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            if record_values:\n",
    "                values.append(info.get(\"portfolio_value\", INITIAL_BALANCE))\n",
    "            \n",
    "            step += 1\n",
    "            if step >= env.max_steps:\n",
    "                break\n",
    "\n",
    "        metrics = env.get_metrics()\n",
    "        metrics[\"portfolio_value\"] = info.get(\"portfolio_value\", INITIAL_BALANCE)\n",
    "        \n",
    "        if record_values:\n",
    "            return metrics, np.array(values)\n",
    "        return metrics, None\n",
    "    \n",
    "    def run(self):\n",
    "        agent_paths = {\n",
    "            \"Q-Learning\": f\"../training_data/checkpoints/exp_qlearning_{TRAIN_VERSION}/final_agent.pkl\",\n",
    "            \"SARSA\": f\"../training_data/checkpoints/exp_sarsa_{TRAIN_VERSION}/final_agent.pkl\",\n",
    "            \"SARSA-λ\": f\"../training_data/checkpoints/exp_sarsa_lambda_{TRAIN_VERSION}/final_agent.pkl\",\n",
    "            \"Monte Carlo\": f\"../training_data/checkpoints/exp_monte_carlo_{TRAIN_VERSION}/final_agent.pkl\",\n",
    "        }\n",
    "\n",
    "        df = df_full.copy()\n",
    "        results = []\n",
    "        portfolio_history = {}\n",
    "        \n",
    "        test_length = len(df) - WINDOW_SIZE - 1\n",
    "\n",
    "        for name, path in agent_paths.items():\n",
    "            print(f\"\\n{name}\")\n",
    "            print(\"-\" * 60)\n",
    "\n",
    "            if not Path(path).exists():\n",
    "                print(f\"Файл не найден: {path}\")\n",
    "                continue\n",
    "\n",
    "            agent = load_agent(path)\n",
    "\n",
    "            env = MyTradingEnv(\n",
    "                df=df.copy(),\n",
    "                initial_balance=INITIAL_BALANCE,\n",
    "                window_size=WINDOW_SIZE,\n",
    "                commission=COMMISSION,\n",
    "                slippage=SLIPPAGE,\n",
    "                max_holding_time=MAX_HOLDING_TIME,\n",
    "                holding_threshold=HOLDING_THRESHOLD,\n",
    "                max_drawdown_threshold=MAX_DRAWDOWN_THRESHOLD,\n",
    "                lambda_drawdown=LAMBDA_DRAWDOWN,\n",
    "                lambda_hold=LAMBDA_HOLD,\n",
    "                reward_scaling=REWARD_SCALING,\n",
    "                max_steps=test_length,  \n",
    "            )\n",
    "            metrics, values = self.run_episode(env, agent, record_values=True)\n",
    "            \n",
    "            if values is not None:\n",
    "                if len(values) != test_length:\n",
    "                    print(f\"Внимание: агент {name} прошел {len(values)} шагов вместо {test_length}\")\n",
    "                    if len(values) < test_length:\n",
    "                        values = np.pad(values, (0, test_length - len(values)), \n",
    "                                       mode='constant', constant_values=values[-1] if len(values) > 0 else INITIAL_BALANCE)\n",
    "                    else:\n",
    "                        values = values[:test_length]\n",
    "            \n",
    "            portfolio_history[name] = values\n",
    "\n",
    "            if values is not None and len(values) > 0:\n",
    "                peak = np.maximum.accumulate(values)\n",
    "                dd = (peak - values) / (peak + 1e-8)\n",
    "                max_dd = dd.max() if len(dd) else 0\n",
    "\n",
    "                returns = np.diff(values) / values[:-1]\n",
    "                sharpe = (\n",
    "                    np.mean(returns) / (np.std(returns) + 1e-8) * np.sqrt(252)\n",
    "                    if len(returns) > 1 else 0\n",
    "                )\n",
    "                \n",
    "                final_value = values[-1] if len(values) > 0 else INITIAL_BALANCE\n",
    "            else:\n",
    "                max_dd = 0\n",
    "                sharpe = 0\n",
    "                final_value = INITIAL_BALANCE\n",
    "\n",
    "            results.append({\n",
    "                \"Агент\": name,\n",
    "                \"Доходность %\": (final_value / INITIAL_BALANCE - 1) * 100,\n",
    "                \"Конечный баланс\": final_value,\n",
    "                \"Сделок\": metrics.get(\"total_trades\", 0),\n",
    "                \"Win Rate %\": metrics.get(\"win_rate\", 0),\n",
    "                \"Средний PnL\": metrics.get(\"avg_pnl\", 0),\n",
    "                \"Max Drawdown сделок %\": metrics.get(\"max_drawdown\", 0) * 100,\n",
    "                \"Max Drawdown портфеля %\": max_dd * 100,\n",
    "                \"Sharpe Ratio\": sharpe,\n",
    "                \"Ср. время удержания\": metrics.get(\"avg_holding_time\", 0),\n",
    "                \"Закрыто по просадке\": metrics.get(\"trades_closed_by_drawdown\", 0),\n",
    "                \"Закрыто по времени\": metrics.get(\"trades_closed_by_time\", 0),\n",
    "            })\n",
    "\n",
    "        plt.figure(figsize=(14, 7))\n",
    "        colors = ['blue', 'green', 'red', 'orange']\n",
    "        \n",
    "        for (name, values), color in zip(portfolio_history.items(), colors):\n",
    "            if values is not None and len(values) > 0:\n",
    "                plt.plot(values, label=name, color=color, linewidth=2)\n",
    "        \n",
    "        plt.title(f\"Рост портфеля агентов ({test_length} шагов)\", fontsize=16)\n",
    "        plt.xlabel(\"Шаг\", fontsize=12)\n",
    "        plt.ylabel(\"Портфель ($)\", fontsize=12)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.legend(fontsize=11)\n",
    "\n",
    "        plt.axhline(y=INITIAL_BALANCE, color='gray', linestyle='--', alpha=0.5, label=f'Начальный баланс: ${INITIAL_BALANCE}')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"portfolio_growth.png\", dpi=100)\n",
    "        plt.show()\n",
    "\n",
    "        df_res = pd.DataFrame(results)\n",
    "        print(\"\\n\" + \"=\"*100)\n",
    "        print(\"РЕЗУЛЬТАТЫ ТЕСТИРОВАНИЯ\")\n",
    "        print(\"=\"*100)\n",
    "        \n",
    "        pd.set_option('display.max_columns', None)\n",
    "        pd.set_option('display.width', None)\n",
    "        pd.set_option('display.float_format', '{:,.2f}'.format)\n",
    "        \n",
    "        print(df_res)\n",
    "        \n",
    "        df_res.to_csv(\"consistent_comparison_results.csv\", index=False)\n",
    "        print(f\"\\nРезультаты сохранены в consistent_comparison_results.csv\")\n",
    "\n",
    "\n",
    "\n",
    "comparator = ConsistentAgentComparator(seed=42)\n",
    "comparator.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
