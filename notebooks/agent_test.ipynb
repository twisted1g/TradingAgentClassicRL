{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d064b844",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from envs.trading_env import MyTradingEnv\n",
    "\n",
    "\n",
    "INITIAL_BALANCE = 1000.0\n",
    "WINDOW_SIZE = 10\n",
    "COMMISSION = 0.0001\n",
    "SLIPPAGE = 0.0005\n",
    "MAX_HOLDING_TIME = 60 * 24\n",
    "HOLDING_THRESHOLD = 24\n",
    "MAX_DRAWDOWN_THRESHOLD = 0.05\n",
    "LAMBDA_DRAWDOWN = 0.1\n",
    "LAMBDA_HOLD = 0.01\n",
    "REWARD_SCALING=10.0\n",
    "MAX_STEPS=None\n",
    "\n",
    "TRAIN_VERSION = \"v2\"\n",
    "\n",
    "df1 = pd.read_csv(\"../data/data_1h_2021.csv\", index_col=0, parse_dates=True)\n",
    "df2 = pd.read_csv(\"../data/data_1h_2022.csv\", index_col=0, parse_dates=True)\n",
    "df3 = pd.read_csv(\"../data/data_1h_2023.csv\", index_col=0, parse_dates=True)\n",
    "df4 = pd.read_csv(\"../data/data_1h_2024.csv\", index_col=0, parse_dates=True)\n",
    "\n",
    "train_data = pd.concat([df1])\n",
    "\n",
    "df_full = train_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a99cd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "class QLearningLoaded:\n",
    "    def __init__(self, data):\n",
    "        self.q_table = data[\"q_table\"]\n",
    "        self.n_actions = data[\"n_actions\"]\n",
    "        self.epsilon = 0.0\n",
    "\n",
    "    def select_action(self, state, training=False):\n",
    "        key = tuple(state)\n",
    "        q = self.q_table.get(key, np.zeros(self.n_actions))\n",
    "        return int(np.argmax(q))\n",
    "\n",
    "\n",
    "class SARSALoaded:\n",
    "    def __init__(self, data):\n",
    "        self.q_table = data[\"q_table\"]\n",
    "        self.n_actions = data[\"n_actions\"]\n",
    "        self.epsilon = 0.0\n",
    "\n",
    "    def select_action(self, state, training=False):\n",
    "        key = tuple(state)\n",
    "        q = self.q_table.get(key, np.zeros(self.n_actions))\n",
    "        return int(np.argmax(q))\n",
    "\n",
    "\n",
    "class SARSALambdaLoaded:\n",
    "    def __init__(self, data):\n",
    "        self.q_table = data[\"q_table\"]\n",
    "        self.n_actions = data[\"n_actions\"]\n",
    "        self.lambda_param = data[\"lambda_param\"]\n",
    "        self.replace_traces = data[\"replace_traces\"]\n",
    "        self.epsilon = 0.0\n",
    "\n",
    "    def select_action(self, state, training=False):\n",
    "        key = tuple(state)\n",
    "        q = self.q_table.get(key, np.zeros(self.n_actions))\n",
    "        return int(np.argmax(q))\n",
    "\n",
    "\n",
    "class MonteCarloLoaded:\n",
    "    def __init__(self, data):\n",
    "        self.q_table = data.get(\"q_table\", {})\n",
    "        self.policy = data.get(\"policy\", {})\n",
    "        self.n_actions = data.get(\"n_actions\", 3)\n",
    "\n",
    "    def select_action(self, state, training=False):\n",
    "        key = tuple(state)\n",
    "        if key in self.policy:\n",
    "            return int(np.argmax(self.policy[key]))\n",
    "        if key in self.q_table:\n",
    "            return int(np.argmax(self.q_table[key]))\n",
    "        return np.random.randint(self.n_actions)\n",
    "\n",
    "\n",
    "def load_agent(path):\n",
    "    with open(path, \"rb\") as f:\n",
    "        agent_instance = pickle.load(f)\n",
    "\n",
    "    name = agent_instance.name.lower()\n",
    "\n",
    "    loaded_data = {\n",
    "        \"q_table\": agent_instance.q_table,\n",
    "        \"n_actions\": agent_instance.n_actions,\n",
    "        \"name\": agent_instance.name,\n",
    "    }\n",
    "\n",
    "    if \"lambda\" in name:\n",
    "        loaded_data[\"lambda_param\"] = getattr(agent_instance, 'lambda_param', None)\n",
    "        loaded_data[\"replace_traces\"] = getattr(agent_instance, 'replace_traces', None)\n",
    "        return SARSALambdaLoaded(loaded_data)\n",
    "        \n",
    "    if \"sarsa\" in name:\n",
    "        return SARSALoaded(loaded_data)\n",
    "        \n",
    "    if \"q\" in name:\n",
    "        return QLearningLoaded(loaded_data)\n",
    "        \n",
    "    if \"monte\" in name:\n",
    "        loaded_data[\"policy\"] = getattr(agent_instance, 'policy', {}) \n",
    "        return MonteCarloLoaded(loaded_data)\n",
    "\n",
    "    raise ValueError(f\"Неизвестный агент: {agent_instance.name}\")\n",
    "\n",
    "\n",
    "class ConsistentAgentComparator:\n",
    "    def __init__(self, seed=42):\n",
    "        self.seed = seed\n",
    "        np.random.seed(seed)\n",
    "        self.test_start = None\n",
    "    \n",
    "    def run_episode(self, env, agent, record_values=False):\n",
    "        state, info = env.reset(seed=self.seed)\n",
    "        done = False\n",
    "        values = []\n",
    "        \n",
    "        if self.test_start is None:\n",
    "            self.test_start = env.current_step\n",
    "        \n",
    "        done = False\n",
    "        step = 0\n",
    "\n",
    "        while not done:\n",
    "            action = agent.select_action(state, training=False)\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "            state = next_state\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            if record_values:\n",
    "                values.append(info.get(\"portfolio_value\", INITIAL_BALANCE))\n",
    "            \n",
    "            step += 1\n",
    "            if step >= env.max_steps:\n",
    "                break\n",
    "\n",
    "        metrics = env.get_metrics()\n",
    "        metrics[\"portfolio_value\"] = info.get(\"portfolio_value\", INITIAL_BALANCE)\n",
    "        \n",
    "        if record_values:\n",
    "            return metrics, np.array(values)\n",
    "        return metrics, None\n",
    "    \n",
    "    def run(self):\n",
    "        agent_paths = {\n",
    "            \"Q-Learning (Base)\": f\"../training_data/checkpoints/exp_qlearning_{TRAIN_VERSION}/final_agent.pkl\",\n",
    "            \"SARSA (Base)\": f\"../training_data/checkpoints/exp_sarsa_{TRAIN_VERSION}/final_agent.pkl\",\n",
    "            \"SARSA-λ (Base)\": f\"../training_data/checkpoints/exp_sarsa_lambda_{TRAIN_VERSION}/final_agent.pkl\",\n",
    "            \"Monte Carlo (Base)\": f\"../training_data/checkpoints/exp_monte_carlo_{TRAIN_VERSION}/final_agent.pkl\",\n",
    "            \"Q-Learning (FT)\": f\"../training_data/checkpoints/exp_qlearning_{TRAIN_VERSION}_finetune/final_agent.pkl\",\n",
    "            \"SARSA (FT)\": f\"../training_data/checkpoints/exp_sarsa_{TRAIN_VERSION}_finetune/final_agent.pkl\",\n",
    "            \"SARSA-λ (FT)\": f\"../training_data/checkpoints/exp_sarsa_lambda_{TRAIN_VERSION}_finetune/final_agent.pkl\",\n",
    "            \"Monte Carlo (FT)\": f\"../training_data/checkpoints/exp_monte_carlo_{TRAIN_VERSION}_finetune/final_agent.pkl\",\n",
    "        }\n",
    "\n",
    "        df = df_full.copy()\n",
    "        results = []\n",
    "        portfolio_history = {}\n",
    "        \n",
    "        test_length = len(df) - WINDOW_SIZE - 1\n",
    "\n",
    "        base_colors = ['blue', 'green', 'red', 'orange']\n",
    "        ft_colors = ['darkblue', 'darkgreen', 'darkred', 'darkorange']\n",
    "        \n",
    "        all_agents = list(agent_paths.keys())\n",
    "        \n",
    "        fig1, axes1 = plt.subplots(2, 2, figsize=(18, 12))\n",
    "        fig1.suptitle('БАЗОВЫЕ АГЕНТЫ: Рост портфеля (Consistent Testing)', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        fig2, axes2 = plt.subplots(2, 2, figsize=(18, 12))\n",
    "        fig2.suptitle('ДООБУЧЕННЫЕ АГЕНТЫ: Рост портфеля (Consistent Testing)', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        fig3, (ax3_base, ax3_ft) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "        fig3.suptitle('СРАВНЕНИЕ ПОРТФЕЛЕЙ ВСЕХ АГЕНТОВ', fontsize=16, fontweight='bold')\n",
    "\n",
    "        base_results = []\n",
    "        ft_results = []\n",
    "        \n",
    "        for i, (name, path) in enumerate(agent_paths.items()):\n",
    "            print(f\"\\n{name}\")\n",
    "            print(\"-\" * 60)\n",
    "\n",
    "            if not Path(path).exists():\n",
    "                print(f\"Файл не найден: {path}\")\n",
    "                continue\n",
    "\n",
    "            agent = load_agent(path)\n",
    "\n",
    "            env = MyTradingEnv(\n",
    "                df=df.copy(),\n",
    "                initial_balance=INITIAL_BALANCE,\n",
    "                window_size=WINDOW_SIZE,\n",
    "                commission=COMMISSION,\n",
    "                slippage=SLIPPAGE,\n",
    "                max_holding_time=MAX_HOLDING_TIME,\n",
    "                holding_threshold=HOLDING_THRESHOLD,\n",
    "                max_drawdown_threshold=MAX_DRAWDOWN_THRESHOLD,\n",
    "                lambda_drawdown=LAMBDA_DRAWDOWN,\n",
    "                lambda_hold=LAMBDA_HOLD,\n",
    "                reward_scaling=REWARD_SCALING,\n",
    "                max_steps=test_length,  \n",
    "            )\n",
    "            metrics, values = self.run_episode(env, agent, record_values=True)\n",
    "            \n",
    "            if values is not None:\n",
    "                if len(values) != test_length:\n",
    "                    print(f\"Внимание: агент {name} прошел {len(values)} шагов вместо {test_length}\")\n",
    "                    if len(values) < test_length:\n",
    "                        values = np.pad(values, (0, test_length - len(values)), \n",
    "                                       mode='constant', constant_values=values[-1] if len(values) > 0 else INITIAL_BALANCE)\n",
    "                    else:\n",
    "                        values = values[:test_length]\n",
    "            \n",
    "            portfolio_history[name] = values\n",
    "\n",
    "            if values is not None and len(values) > 0:\n",
    "                peak = np.maximum.accumulate(values)\n",
    "                dd = (peak - values) / (peak + 1e-8)\n",
    "                max_dd = dd.max() if len(dd) else 0\n",
    "\n",
    "                returns = np.diff(values) / values[:-1]\n",
    "                sharpe = (\n",
    "                    np.mean(returns) / (np.std(returns) + 1e-8) * np.sqrt(252)\n",
    "                    if len(returns) > 1 else 0\n",
    "                )\n",
    "                \n",
    "                final_value = values[-1] if len(values) > 0 else INITIAL_BALANCE\n",
    "            else:\n",
    "                max_dd = 0\n",
    "                sharpe = 0\n",
    "                final_value = INITIAL_BALANCE\n",
    "\n",
    "            result_entry = {\n",
    "                \"Агент\": name,\n",
    "                \"Доходность %\": (final_value / INITIAL_BALANCE - 1) * 100,\n",
    "                \"Конечный баланс\": final_value,\n",
    "                \"Сделок\": metrics.get(\"total_trades\", 0),\n",
    "                \"Win Rate %\": metrics.get(\"win_rate\", 0),\n",
    "                \"Средний PnL\": metrics.get(\"avg_pnl\", 0),\n",
    "                \"Max Drawdown сделок %\": metrics.get(\"max_drawdown\", 0) * 100,\n",
    "                \"Max Drawdown портфеля %\": max_dd * 100,\n",
    "                \"Sharpe Ratio\": sharpe,\n",
    "                \"Ср. время удержания\": metrics.get(\"avg_holding_time\", 0),\n",
    "                \"Закрыто по просадке\": metrics.get(\"trades_closed_by_drawdown\", 0),\n",
    "                \"Закрыто по времени\": metrics.get(\"trades_closed_by_time\", 0),\n",
    "            }\n",
    "            \n",
    "            results.append(result_entry)\n",
    "            \n",
    "            if \"(FT)\" in name:\n",
    "                ft_results.append(result_entry)\n",
    "                agent_idx = list(agent_paths.keys()).index(name)\n",
    "                base_idx = agent_idx - 4\n",
    "                color_idx = base_idx % 4\n",
    "                \n",
    "                if color_idx < len(ft_colors):\n",
    "                    color = ft_colors[color_idx]\n",
    "                else:\n",
    "                    color = ft_colors[color_idx % len(ft_colors)]\n",
    "                    \n",
    "                ax_idx = color_idx\n",
    "                row = ax_idx // 2\n",
    "                col = ax_idx % 2\n",
    "                \n",
    "                if values is not None and len(values) > 0:\n",
    "                    axes2[row, col].plot(values, label=name, color=color, linewidth=2)\n",
    "                    axes2[row, col].set_title(name.split(' (')[0], fontsize=14)\n",
    "                    axes2[row, col].set_xlabel(\"Шаг\", fontsize=10)\n",
    "                    axes2[row, col].set_ylabel(\"Портфель ($)\", fontsize=10)\n",
    "                    axes2[row, col].grid(True, alpha=0.3)\n",
    "                    axes2[row, col].axhline(y=INITIAL_BALANCE, color='gray', linestyle='--', alpha=0.5)\n",
    "                    \n",
    "                ax3_ft.plot(values, label=name, color=color, linewidth=2)\n",
    "            else:\n",
    "                base_results.append(result_entry)\n",
    "                color_idx = i % 4\n",
    "                \n",
    "                if color_idx < len(base_colors):\n",
    "                    color = base_colors[color_idx]\n",
    "                else:\n",
    "                    color = base_colors[color_idx % len(base_colors)]\n",
    "                    \n",
    "                ax_idx = color_idx\n",
    "                row = ax_idx // 2\n",
    "                col = ax_idx % 2\n",
    "                \n",
    "                if values is not None and len(values) > 0:\n",
    "                    axes1[row, col].plot(values, label=name, color=color, linewidth=2)\n",
    "                    axes1[row, col].set_title(name, fontsize=14)\n",
    "                    axes1[row, col].set_xlabel(\"Шаг\", fontsize=10)\n",
    "                    axes1[row, col].set_ylabel(\"Портфель ($)\", fontsize=10)\n",
    "                    axes1[row, col].grid(True, alpha=0.3)\n",
    "                    axes1[row, col].axhline(y=INITIAL_BALANCE, color='gray', linestyle='--', alpha=0.5)\n",
    "                    \n",
    "                ax3_base.plot(values, label=name, color=color, linewidth=2)\n",
    "\n",
    "        for i in range(2):\n",
    "            for j in range(2):\n",
    "                axes1[i, j].legend(fontsize=9)\n",
    "                axes2[i, j].legend(fontsize=9)\n",
    "        \n",
    "        ax3_base.set_title(\"Базовые агенты\", fontsize=14)\n",
    "        ax3_base.set_xlabel(\"Шаг\", fontsize=12)\n",
    "        ax3_base.set_ylabel(\"Портфель ($)\", fontsize=12)\n",
    "        ax3_base.grid(True, alpha=0.3)\n",
    "        ax3_base.legend(fontsize=10)\n",
    "        ax3_base.axhline(y=INITIAL_BALANCE, color='gray', linestyle='--', alpha=0.5, label=f'Начальный баланс: ${INITIAL_BALANCE}')\n",
    "        \n",
    "        ax3_ft.set_title(\"Дообученные агенты\", fontsize=14)\n",
    "        ax3_ft.set_xlabel(\"Шаг\", fontsize=12)\n",
    "        ax3_ft.set_ylabel(\"Портфель ($)\", fontsize=12)\n",
    "        ax3_ft.grid(True, alpha=0.3)\n",
    "        ax3_ft.legend(fontsize=10)\n",
    "        ax3_ft.axhline(y=INITIAL_BALANCE, color='gray', linestyle='--', alpha=0.5, label=f'Начальный баланс: ${INITIAL_BALANCE}')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        df_res = pd.DataFrame(results)\n",
    "        print(\"\\n\" + \"=\"*120)\n",
    "        print(\"РЕЗУЛЬТАТЫ ТЕСТИРОВАНИЯ ВСЕХ АГЕНТОВ\")\n",
    "        print(\"=\"*120)\n",
    "        \n",
    "        pd.set_option('display.max_columns', None)\n",
    "        pd.set_option('display.width', None)\n",
    "        pd.set_option('display.float_format', '{:,.2f}'.format)\n",
    "        \n",
    "        print(df_res)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*120)\n",
    "        print(\"ТОП-3 ПО ДОХОДНОСТИ\")\n",
    "        print(\"=\"*120)\n",
    "        \n",
    "        top_return = df_res.sort_values(\"Доходность %\", ascending=False).head(3)\n",
    "        print(top_return[[\"Агент\", \"Доходность %\", \"Конечный баланс\", \"Sharpe Ratio\", \"Max Drawdown портфеля %\"]])\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*120)\n",
    "        print(\"ТОП-3 ПО SHARPE RATIO\")\n",
    "        print(\"=\"*120)\n",
    "        \n",
    "        top_sharpe = df_res.sort_values(\"Sharpe Ratio\", ascending=False).head(3)\n",
    "        print(top_sharpe[[\"Агент\", \"Sharpe Ratio\", \"Доходность %\", \"Конечный баланс\", \"Max Drawdown портфеля %\"]])\n",
    "        \n",
    "        df_res.to_csv(\"consistent_comparison_results_all.csv\", index=False)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*120)\n",
    "        print(\"СРАВНЕНИЕ BASE vs FINETUNE\")\n",
    "        print(\"=\"*120)\n",
    "        \n",
    "        comparison_data = []\n",
    "        agent_types = [\"Q-Learning\", \"SARSA\", \"SARSA-λ\", \"Monte Carlo\"]\n",
    "        \n",
    "        for agent_type in agent_types:\n",
    "            base_agent = f\"{agent_type} (Base)\"\n",
    "            ft_agent = f\"{agent_type} (FT)\"\n",
    "            \n",
    "            base_data = df_res[df_res[\"Агент\"] == base_agent]\n",
    "            ft_data = df_res[df_res[\"Агент\"] == ft_agent]\n",
    "            \n",
    "            if not base_data.empty and not ft_data.empty:\n",
    "                base_row = base_data.iloc[0]\n",
    "                ft_row = ft_data.iloc[0]\n",
    "                \n",
    "                return_diff = ft_row[\"Доходность %\"] - base_row[\"Доходность %\"]\n",
    "                sharpe_diff = ft_row[\"Sharpe Ratio\"] - base_row[\"Sharpe Ratio\"]\n",
    "                dd_diff = ft_row[\"Max Drawdown портфеля %\"] - base_row[\"Max Drawdown портфеля %\"]\n",
    "                \n",
    "                comparison_data.append({\n",
    "                    \"Тип агента\": agent_type,\n",
    "                    \"Доходность Base\": base_row[\"Доходность %\"],\n",
    "                    \"Доходность FT\": ft_row[\"Доходность %\"],\n",
    "                    \"Δ Доходность\": return_diff,\n",
    "                    \"Sharpe Base\": base_row[\"Sharpe Ratio\"],\n",
    "                    \"Sharpe FT\": ft_row[\"Sharpe Ratio\"],\n",
    "                    \"Δ Sharpe\": sharpe_diff,\n",
    "                    \"MDD Base\": base_row[\"Max Drawdown портфеля %\"],\n",
    "                    \"MDD FT\": ft_row[\"Max Drawdown портфеля %\"],\n",
    "                    \"Δ MDD\": dd_diff,\n",
    "                })\n",
    "        \n",
    "        df_comparison = pd.DataFrame(comparison_data)\n",
    "        print(df_comparison.round(2))\n",
    "        df_comparison.to_csv(\"base_vs_finetune_comparison.csv\", index=False)\n",
    "        \n",
    "        print(f\"\\nРезультаты сохранены в consistent_comparison_results_all.csv и base_vs_finetune_comparison.csv\")\n",
    "\n",
    "comparator = ConsistentAgentComparator(seed=42)\n",
    "comparator.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
